[![readme](https://img.shields.io/badge/README-blue)](/)
[![part02](https://img.shields.io/badge/0051--0100-blue)](/questions/0001-0050.md)
[![part02](https://img.shields.io/badge/0051--0100-blue)](/questions/0051-0100.md)
[![part03](https://img.shields.io/badge/0101--0150-blue)](/questions/0101-0150.md)
[![part04](https://img.shields.io/badge/0151--0200-blue)](/questions/0151-0200.md)
[![part05](https://img.shields.io/badge/0201--0250-blue)](/questions/0201-0250.md)
[![part06](https://img.shields.io/badge/0251--0300-blue)](/questions/0251-0300.md)
[![part07](https://img.shields.io/badge/0301--0350-blue)](/questions/0301-0350.md)
[![part08](https://img.shields.io/badge/0351--0400-blue)](/questions/0351-0400.md)
[![part09](https://img.shields.io/badge/0401--0450-blue)](/questions/0401-0450.md)
[![part10](https://img.shields.io/badge/0451--0500-blue)](/questions/0451-0500.md)
[![part11](https://img.shields.io/badge/0501--0550-blue)](/questions/0501-0550.md)
[![part12](https://img.shields.io/badge/0551--0600-blue)](/questions/0551-0600.md)
[![part13](https://img.shields.io/badge/0601--0650-blue)](/questions/0601-0650.md)
[![part14](https://img.shields.io/badge/0651--0700-blue)](/questions/0651-0700.md)
[![part15](https://img.shields.io/badge/0701--0750-blue)](/questions/0701-0750.md)
[![part16](https://img.shields.io/badge/0751--0800-blue)](/questions/0751-0800.md)
[![part17](https://img.shields.io/badge/0801--0850-blue)](/questions/0801-0850.md)
[![part18](https://img.shields.io/badge/0851--0900-blue)](/questions/0851-0900.md)
[![part19](https://img.shields.io/badge/0901--0950-blue)](/questions/0901-0950.md)
[![part20](https://img.shields.io/badge/0951--1000-blue)](/questions/0951-1000.md)
[![part21](https://img.shields.io/badge/1001--1050-blue)](/questions/1001-1050.md)
[![part22](https://img.shields.io/badge/1051--1100-blue)](/questions/1051-1100.md)
[![part23](https://img.shields.io/badge/1101--1150-blue)](/questions/1101-1150.md)
[![part24](https://img.shields.io/badge/1151--1203-blue)](/questions/1151-1203.md)



### Question 51

Let's examine each option for a version control system that supports collaborative software development with the specified requirements:

#### A. AWS CodePipeline: AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service, not a version control system. It helps automate the build, test, and deployment phases of your application.

#### B. Amazon S3: Amazon S3 is an object storage service and is not a version control system. While it can be used to store files and data, it doesn't provide version tracking, parallel branching, and features typically found in version control systems.

#### C. AWS CodeBuild: AWS CodeBuild is a build service for compiling source code, running tests, and producing software packages. It is not a version control system and doesn't provide features for version tracking, parallel branching, or change tracking.

#### D. AWS CodeCommit: AWS CodeCommit is a fully managed source control service that provides Git-based version control. It supports version tracking, parallel branching (with Git's native branching capabilities), and change tracking across multiple files. It is designed for collaborative software development and meets the specified requirements.

Therefore, the AWS service that meets the requirements for collaborative software development with version tracking, parallel branching, and support for batches of changes across multiple files is option D: AWS CodeCommit.

---

### Question 52

Let's examine each option to see which managed AWS services enable asynchronous message passing in a microservices architecture:

#### A. Amazon SQS (Simple Queue Service): Amazon SQS is a fully managed message queuing service that decouples the components of a cloud application, allowing them to communicate asynchronously. It's designed for sending and receiving messages between distributed software components. Developers can use it to facilitate communication between microservices without impacting performance.

#### B. Amazon Cognito: Amazon Cognito is an identity management service and doesn't directly provide message-passing capabilities for microservices communication. It's primarily used for user authentication and authorization.

#### C. Amazon Kinesis: Amazon Kinesis is a platform for streaming data on AWS, designed for real-time data processing. It's not primarily focused on asynchronous message passing between microservices, but rather on managing and analyzing streaming data.

#### D. Amazon SNS (Simple Notification Service): Amazon SNS is a fully managed notification service that can be used to fan out messages to a variety of distributed endpoints, including Amazon SQS queues, Lambda functions, HTTP endpoints, and more. It allows for asynchronous messaging and is a suitable option for microservices communication.

#### E. Amazon ElastiCache: Amazon ElastiCache is a fully managed, in-memory caching service used to speed up data access. While it can improve performance in microservices architecture, it's not directly related to enabling asynchronous message passing.

Based on the explanations, the managed AWS services that enable asynchronous message passing for microservices in a decoupled architecture are:

#### A. Amazon SQS (Simple Queue Service)
D. Amazon SNS (Simple Notification Service)

These services allow microservices to communicate asynchronously and efficiently, which is essential in a microservices-based architecture.

---

### Question 53

Let's examine each option to see which techniques work for repeatedly and consistently deploying a serverless RESTful API on AWS:

#### A. Define a Swagger file. Use AWS Elastic Beanstalk to deploy the Swagger file:

AWS Elastic Beanstalk is a Platform as a Service (PaaS) that is suitable for deploying web applications. However, using it with Swagger files is not a common approach for deploying serverless RESTful APIs. Elastic Beanstalk is more commonly associated with applications using a traditional server-based model.

#### B. Define a Swagger file. Use AWS CodeDeploy to deploy the Swagger file:

AWS CodeDeploy is a service for automating code deployments, but it is typically used for deploying application code, not API definitions like Swagger files. This is not a standard method for deploying serverless RESTful APIs.

#### C. Deploy a SAM template with an inline Swagger definition:

AWS SAM (Serverless Application Model) templates can be used to define serverless applications, including RESTful APIs. However, inline Swagger definitions are not typically used for describing RESTful APIs within a SAM template. SAM itself provides simplified constructs for defining APIs. Posted On Feb 17, 2017: You can now specify Swagger inline using the new definitionBody property instead of having to include a separate Swagger file with your SAM file.

#### D. Define a Swagger file. Deploy a SAM template that references the Swagger file:

This is a valid approach. You can define your RESTful API in a Swagger file and then deploy it using a SAM (Serverless Application Model) template. The SAM template can reference the Swagger file and define the API Gateway, Lambda functions, and other resources required for your serverless API.

#### E. Define an inline Swagger definition in a Lambda function. Invoke the Lambda function:

This approach is not typically used for deploying a serverless RESTful API. While you can define an API inline within a Lambda function, this method is not efficient or scalable for managing RESTful APIs.

Based on the explanations, the techniques that work for repeatedly and consistently deploying a serverless RESTful API on AWS are:

#### D. Define a Swagger file. Deploy a SAM template that references the Swagger file.
(Using AWS SAM templates to define and deploy your RESTful API)

Additionally, the correct approach would involve using AWS API Gateway along with AWS Lambda to define and deploy your serverless RESTful API.

---

### Question 54

Let's break down each option to see which approach the Developer should use to handle the change when deploying a new version (v2) of an API without impacting existing clients:

#### A. Update the underlying Lambda function and provide clients with the new Lambda invocation URL:

This option doesn't provide a clean separation between API versions. Clients would need to update their endpoint URLs, which can be challenging. It also does not take advantage of API Gateway's features for versioning and managing multiple stages.

#### B. Use API Gateway to automatically propagate the change to clients, specifying 180 days in the phased deployment parameter:

While API Gateway allows for easy management of different API versions, it doesn't automatically handle the migration of clients over a specific timeframe. You would typically need to set up a new stage and encourage clients to migrate to it.

#### C. Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL:

This is the recommended approach for handling API versioning and migrations with Amazon API Gateway. You can create a new stage (e.g., v2) for the updated API and provide users with the new stage URL. This allows clients to migrate at their own pace without affecting existing users.

#### D. Update the underlying Lambda function, and create an Amazon CloudFront distribution with the updated Lambda function as its origin:

While Amazon CloudFront can improve the performance and scalability of API Gateway by caching, this approach doesn't provide a versioning strategy or a clean transition path for clients to move from v1 to v2 of the API. It's more about performance optimization than version management.

The correct approach for handling the change when deploying a new version (v2) of the API without impacting existing clients is:

#### C. Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL.
(Creating a new stage for the updated API and allowing users to migrate at their own pace)

---

### Question 55

Let's analyze each option for achieving the ability to roll back to a previous version of an AWS Lambda function with minimal impact on users:

#### A. Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version:

This approach uses AWS Lambda aliases, allowing you to easily switch between versions using an alias. It minimizes the impact on users as you can quickly switch back to the previous version in case of errors.

#### B. Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version:

This approach uses alias weighting to gradually shift traffic to the new version. If too many errors are encountered, you can quickly roll back all users to the previous version. This can help mitigate issues with MINIMAL user IMPACT.

#### C. Do not make any changes to the application. Deploy the new version of the code. If too many errors are encountered, point the application back to the previous version using the version number in the Amazon Resource Name (ARN):

This approach does not utilize AWS Lambda aliases but relies on manually changing the version number in the ARN. It may not be the most practical solution, as it doesn't offer as much flexibility or automation for rolling back.

#### D. Create three aliases: new, existing, and router. Point the existing alias to the current version. Have the router alias direct 100% of users to the existing alias. Update the application to use the router alias. Deploy the new version of the code. Point the new alias to this version. Update the router alias to direct 10% of users to the new alias. If too many errors are encountered, send 100% of traffic to the existing alias:

This approach involves a more complex setup with three aliases, including a "router" alias that controls traffic distribution. While it allows for a gradual transition and quick rollback, it may be considered over-engineered for some scenarios.

The correct approach for achieving the ability to roll back to a previous version of an AWS Lambda function with minimal impact on users is:

#### A. Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version.
(This approach is practical and provides a straightforward way to roll back to a previous version using AWS Lambda aliases.)

---

### Question 57

Let's break down each option for deploying the application on AWS Elastic Beanstalk based on the requirements of having two components that need to scale independently.

#### A. Deploy the application in a single Elastic Beanstalk environment:

In this option, both components would be deployed together in a single Elastic Beanstalk environment. This would mean that they share the same environment resources, configurations, and scaling settings. If one component needs to scale independently, it may not be possible or would be challenging to do so. This option does not meet the requirement of independently scaling the components and is not recommended.

#### B. Deploy each component in a separate Elastic Beanstalk environment:

This option involves deploying each component (HTTP requests and background processing tasks) into separate Elastic Beanstalk environments. Each environment can have its own scaling settings, configurations, and resources. This approach ensures that each component can scale independently, and it's the most suitable choice for the given requirements.

#### C. Use multiple Elastic Beanstalk environments for the HTTP component but one environment for the background task component:

In this option, only the HTTP component is deployed in multiple Elastic Beanstalk environments, allowing it to scale independently. However, the background task component is deployed in a single environment. This doesn't meet the requirement of independent scaling for both components. It's better to have both components in separate environments if they need to scale independently.

#### D. Use multiple Elastic Beanstalk environments for the background task component but one environment for the HTTP component:

Similar to option C, this approach deploys only one component in multiple environments. While it allows the background task component to scale independently, it doesn't provide the same benefit for the HTTP component. It's important that both components have the flexibility to scale independently to meet the application's requirements.

So, the recommended choice to meet the requirement of having two components that must scale independently is:

#### B. Deploy each component in a separate Elastic Beanstalk environment.

---

### Question 58

Let's analyze each option for securely storing data in Amazon S3 while meeting the encryption and key management requirements:

#### A. Implement server-side encryption using customer-provided encryption keys (SSE-C):

In this approach, the developer provides their own encryption keys to Amazon S3 to encrypt the data on the server side. The developer would manage the encryption keys and provide them to S3 when uploading objects. However, since the management requirement is for the Security team to manage the encryption keys, this option does not meet that requirement. The developer would retain control over the encryption keys.

#### B. Implement server-side encryption by using a client-side master key:

This approach typically involves using a client-side library or tool to encrypt data on the client side before sending it to S3. The client-side master key, in this case, would be managed by the Security team, which aligns with the management requirement. However, this option is not a standard AWS offering and would require custom client-side encryption logic to be implemented.

#### C. Implement client-side encryption using an AWS KMS-managed customer master key (CMK):

In this approach, AWS Key Management Service (KMS) manages the customer master key (CMK), which is used for client-side encryption. The CMK can be controlled by the Security team, ensuring that encryption keys are managed according to the requirement. Data is encrypted client-side, and the encrypted data is then uploaded to S3. This approach is both secure and compliant with AWS's recommended practices.

#### D. Implement client-side encryption using Amazon S3-managed keys:

Amazon S3-managed keys are used for server-side encryption, and they do not meet the requirement of having the Security team manage the encryption keys. The keys are managed by S3 itself, and the developer does not have control over them.

The best approach to meet the requirement of encrypting data in Amazon S3 and having the encryption keys managed by the Security team is:

#### C. Implement client-side encryption using an AWS KMS-managed customer master key (CMK).

---

### Question 59

When you need to find out how changes in an AWS CloudFormation stack will impact the resources that are running, you can investigate various aspects of your CloudFormation templates. Let's go through the options:

#### A. Investigate the change sets:

Change sets are a crucial feature in AWS CloudFormation that allow you to preview the changes that will occur when you update a stack. Change sets provide a detailed view of the resources that will be created, updated, or deleted during the update. By investigating change sets, you can review the impact of the changes before executing them. This is the correct option for assessing the impact of changes in a CloudFormation stack.

#### B. Investigate the stack policies:

Stack policies in AWS CloudFormation are used to control updates to stacks. They define the update actions allowed or denied for various stack resources. While stack policies are essential for controlling changes, they don't directly provide information about the impact of changes. They are more about controlling the changes, not evaluating their impact.

#### C. Investigate the Metadata section:

The Metadata section in a CloudFormation template is typically used for providing additional information and documentation about your resources. It doesn't directly offer insights into how changes will impact resources. It's more for annotation and documentation purposes.

#### D. Investigate the Resources section:

The Resources section is where you define the AWS resources that make up your stack. While it's essential to understand what resources are defined in your template, it doesn't provide a direct way to assess the impact of changes. To do that, you would use change sets, as mentioned in option A.

So, the correct option to find out how the changes will impact the resources when updating an AWS CloudFormation stack is:

#### A. Investigate the change sets.

---

### Question 60

To efficiently perform code pushes without the need to update the Amazon API Gateway target endpoint in a serverless web application with different branches of code, you need to consider the following options:

#### A. Associate different AWS Lambda functions to an API Gateway target endpoint:

This approach involves creating multiple AWS Lambda functions, each associated with a different branch of code. You can then configure your API Gateway to use these different Lambda functions as target endpoints. While this would work, it can become complex and difficult to manage as you add more branches. This approach might require frequent updates to the API Gateway, which is what you're trying to avoid.

#### B. Create different stages in API Gateway, then associate API Gateway with AWS Lambda:

This approach is more efficient for managing different code branches without updating the API Gateway frequently. You can create different stages in API Gateway (e.g., development, staging, production), and associate each stage with a specific version or alias of the AWS Lambda function. As you deploy new versions or aliases of your Lambda function, you can easily switch the stage to point to the desired version or alias without modifying the API Gateway configuration. This is a good practice for managing different code branches efficiently and is often used in real-world scenarios.

#### C. Create aliases and versions in AWS Lambda:

While creating aliases and versions in AWS Lambda is a key part of the solution, this alone won't prevent the need to update the API Gateway. You can use aliases and versions to manage different code branches and deploy new code versions, but the API Gateway needs to be configured to use these Lambda versions and aliases, as discussed in option B.

#### D. Tag the AWS Lambda functions with different names:

Tagging Lambda functions with different names won't directly address the problem of avoiding updates to the API Gateway when performing code pushes. You would still need to manually update the API Gateway to point to the correct Lambda function with the desired tag, which is not an efficient solution.

The most suitable solution to efficiently perform code pushes without frequently updating the Amazon API Gateway is:

#### B. Create different stages in API Gateway, then associate API Gateway with AWS Lambda.

---

### Question 61

To ensure that all traffic to an Amazon S3 bucket is encrypted in transit, you should use the appropriate S3 bucket and object policies, as well as configuring your EC2 instances. Let's go through each option:

#### A. Install certificates on the EC2 instances:

Installing certificates on the EC2 instances is a common practice for securing traffic to and from web servers running on these instances. However, in this context, it doesn't ensure that traffic to the S3 bucket is encrypted. This option doesn't address data transfers between the EC2 instances and the S3 bucket, so it's not a sufficient solution.

#### B. Create a bucket policy that allows traffic where SecureTransport is true:

While it's technically possible to create a bucket policy, there is no built-in "SecureTransport" condition key that can be used to enforce encryption in transit for S3 traffic. This is not a valid or effective solution.

#### C. Create an HTTPS redirect on the EC2 instances:

Creating an HTTPS redirect on the EC2 instances would ensure that any HTTP requests to your EC2 instances are redirected to HTTPS, securing the communication between clients and your application on the EC2 instances. However, this does not directly impact traffic to and from the S3 bucket. It's a good security practice for web servers but doesn't address S3 encryption in transit.

#### D. Create a bucket policy that denies traffic where SecureTransport is false:

This option refers to the use of a bucket policy to deny S3 access requests that do not use SSL/TLS (SecureTransport). While the wording of "SecureTransport" might not be standard, the concept is valid. You can use bucket policies to enforce the use of SSL/TLS (encryption in transit) for S3 data transfers. By specifying a condition in the bucket policy that checks whether the request is over HTTPS, you can effectively deny unencrypted traffic. This is a common approach for ensuring S3 data is encrypted in transit.

The correct option for ensuring that all traffic to the S3 bucket is encrypted in transit is:

#### D. Create a bucket policy that denies traffic where SecureTransport is false.

---

### Question 62

To meet the requirement of creating a RESTful API for customers to query the status of orders with the API endpoint http://www.supplierdomain.com/status/customerID, you need an architecture that can handle HTTP requests and route them to the appropriate logic for processing. Let's evaluate each option:

#### A. Amazon SQS; Amazon SNS:

Amazon SQS (Simple Queue Service) and Amazon SNS (Simple Notification Service) are messaging services used for decoupling applications and managing message-based communication. They are not designed for building RESTful APIs. These services don't directly create HTTP endpoints for customers to query order status, so they do not meet the requirement.

#### B. Elastic Load Balancing; Amazon EC2:

Elastic Load Balancing is used to distribute incoming application or network traffic across multiple Amazon EC2 instances. While this is a valid infrastructure component for managing web traffic, it doesn't directly create RESTful APIs. Amazon EC2 instances can host applications, but additional components are required for building a RESTful API.

#### C. Amazon ElastiCache; Amazon Elasticsearch Service:

Amazon ElastiCache is a caching service, and Amazon Elasticsearch Service is used for search and analytics. These services do not directly facilitate building RESTful APIs. They are generally used for improving the performance of applications and analyzing data but do not provide API endpoints.

#### D. Amazon API Gateway; AWS Lambda:

Amazon API Gateway is a fully managed service for creating, publishing, and securing RESTful APIs. AWS Lambda is a serverless compute service that can run code in response to API Gateway requests. This combination is well-suited for creating RESTful APIs. You can use API Gateway to define your API endpoints (such as http://www.supplierdomain.com/status/customerID) and route requests to AWS Lambda functions that process and provide responses. This is a suitable choice for meeting the requirements.

#### E. Amazon S3; Amazon CloudFront:

Amazon S3 and Amazon CloudFront can be used for serving static web content and accelerating content delivery through a content delivery network (CDN). However, this combination does not directly provide the ability to create custom RESTful APIs. It's more suitable for serving static assets like web pages, images, or videos.

The two options that meet the requirements for creating a RESTful API for customers are:

#### B. Elastic Load Balancing; Amazon EC2.
D. Amazon API Gateway; AWS Lambda.

---

### Question 63

When designing an AWS Lambda function that creates temporary files during execution, especially when the files are small (less than 10 MB) and don't need to be saved or retrieved in the future, you need to consider the appropriate storage option. Let's evaluate each option:

#### A. the /tmp directory:

AWS Lambda provides a temporary file system in the `/tmp` directory that is ideal for storing temporary files during function execution. The `/tmp` directory has limited storage capacity (512 MB) and is local to the execution environment. This storage is suitable for temporary files like caching or intermediate data that do not need to persist between Lambda function invocations. It is a cost-effective and practical choice for your use case.

#### B. Amazon EFS (Elastic File System):

Amazon EFS is a scalable and managed file storage service that can be mounted on multiple EC2 instances and containers. However, using EFS for temporary files in Lambda may be overkill for your use case, as it is designed for scenarios where you need shared and persistent file storage across multiple instances or containers.

#### C. Amazon EBS (Elastic Block Store):

Amazon EBS is a block storage service used to provide persistent storage for EC2 instances. It is not directly tied to AWS Lambda functions, and using EBS for temporary files in Lambda is not a standard practice. EBS is designed for persistent data storage, which is not necessary for temporary files in Lambda.

#### D. Amazon S3:

Amazon S3 is an object storage service that is suitable for storing large volumes of data, but it is not the best option for storing temporary files in a Lambda function. S3 is designed for durable and scalable storage, and interacting with it in a Lambda function introduces additional complexity and potential latency, which may not be necessary for your use case.

The recommended and cost-effective option for storing temporary files during AWS Lambda function execution is:

#### A. the /tmp directory.

---

### Question 64

When addressing the issue of increasing page load times due to user profile data retrieval from a database, you need an efficient caching strategy. Let's evaluate each option:

#### A. Create a new Amazon EC2 Instance and run a NoSQL database on it. Cache the profile data within this database using the write-through caching strategy:

This option suggests deploying a separate EC2 instance running a NoSQL database and implementing write-through caching. While this approach could improve performance, it introduces added complexity, management overhead, and costs due to maintaining a separate database instance. This may not be the most efficient solution for addressing the issue.

#### B. Create an Amazon ElastiCache cluster to cache the user profile data. Use a cache-aside caching strategy:

Amazon ElastiCache is a managed caching service that is specifically designed for caching. Using ElastiCache to cache user profile data is a more efficient and recommended approach. The cache-aside (lazy loading) strategy is suitable for this use case. When a user profile is requested, it is fetched from the cache if available, and if not, it's fetched from the database and then stored in the cache for subsequent requests. This approach minimizes the load on the database and improves page load times.

#### C. Use a dedicated Amazon RDS instance for caching profile data. Use a write-through caching strategy:

Using an RDS instance for caching is not a typical use case for RDS. Write-through caching is generally implemented with services like Amazon ElastiCache. This option might not be as efficient or cost-effective as using a caching service specifically designed for this purpose.

#### D. Create an ElastiCache cluster to cache the user profile data. Use a write-through caching strategy:

This option is similar to option B but suggests using a write-through caching strategy with ElastiCache. While write-through caching can be effective, it's usually used with databases that need to be updated immediately when data changes. In this context, cache-aside (lazy loading) is often more efficient, as it minimizes the database load and allows the cache to be populated gradually based on user requests.

The most efficient and recommended caching strategy for this situation is:

#### B. Create an Amazon ElastiCache cluster to cache the user profile data. Use a cache-aside caching strategy.

---

### Question 65

To meet the requirement of migrating an advertising company's dynamic website infrastructure to AWS while not handling website development, you need a solution that is both scalable and easy to manage. Let's evaluate each option:

#### A. Use AWS VM Import to migrate a web server image to AWS. Launch the image on a compute-optimized Amazon EC2 instance:

While this option allows you to bring an existing virtual machine image into AWS, it doesn't provide the scalability and management ease needed for a high-traffic website. Managing a single EC2 instance can be complex, and scaling with this approach can be challenging.

#### B. Launch multiple Amazon Lightsail instances behind a load balancer. Set up the website in those instances:

Amazon Lightsail is a simplified service designed for smaller-scale projects. It's not the most suitable choice for heavy-traffic, dynamic websites. Using Lightsail instances would not provide the scalability, automation, and management features that a high-traffic website requires.

#### C. Deploy the website code in an AWS Elastic Beanstalk environment. Use Auto Scaling to scale the number of instances:

This option involves deploying the website code in an AWS Elastic Beanstalk environment, which is a Platform as a Service (PaaS) offering. Elastic Beanstalk makes it easy to deploy, manage, and scale web applications. You can leverage Auto Scaling to automatically adjust the number of instances based on traffic load, ensuring scalability. This is a suitable choice for managing a dynamic website infrastructure.

#### D. Use Amazon S3 to host the website. Use Amazon CloudFront to deliver the content at scale:

This approach involves hosting the static content of the website in Amazon S3 and using Amazon CloudFront as a content delivery network (CDN). While this is a great way to efficiently serve static content and reduce latency, it doesn't handle the dynamic aspects of the website. You would still need to manage a web server or application layer for dynamic content, which complicates the solution.

The best solution that meets the requirements of handling a dynamic website with heavy traffic while not needing to manage website development is:

#### C. Deploy the website code in an AWS Elastic Beanstalk environment. Use Auto Scaling to scale the number of instances.

---

### Question 66

To log key events that occur during an AWS Lambda function and include a unique identifier to associate the events with a specific function invocation, you need to understand the options available for logging in AWS Lambda and where you can obtain the unique identifier. Let's evaluate each option:

#### A. Obtain the request identifier from the Lambda context object. Architect the application to write logs to the console:

The Lambda context object provides various information about the current invocation, including request-specific information like the request identifier. Writing logs to the console (standard output) is a common way to log events in Lambda. However, this approach may not be suitable for capturing events in a structured and easily searchable format, which can be challenging to work with. The console logs can be captured and persisted using AWS CloudWatch Logs, making them accessible for later analysis and troubleshooting.

#### B. Obtain the request identifier from the Lambda event object. Architect the application to write logs to a file:

This option suggests obtaining the request identifier from the Lambda event object, which may be possible depending on the structure of the event source. Writing logs to a file is a more structured approach compared to console logging. It allows you to capture events in a file, which can be stored in an S3 bucket or other storage for further analysis. This option is more suitable for detailed logging.

#### C. Obtain the request identifier from the Lambda event object. Architect the application to write logs to the console:

This option suggests obtaining the request identifier from the Lambda event object and writing logs to the console. While writing logs to the console is a straightforward way to capture events, it may not be the most efficient option for collecting and analyzing logs, especially when dealing with large volumes of data.

#### D. Obtain the request identifier from the Lambda context object. Architect the application to write logs to a file:

This option recommends obtaining the request identifier from the Lambda context object, which is generally used for request-specific details. Writing logs to a file is a structured and more efficient approach for capturing and storing logs, as it allows for easier analysis and storage options. You can store them in Amazon S3, EFS, or other storage solutions, and you can also configure log rotation and retention as needed.

Both option A and option D can be valid choices, depending on your specific use case and requirements. If you prefer the flexibility of structured logging, long-term storage, or more control over log management, then option D might be more suitable. However, if you are comfortable with CloudWatch Logs and are looking for a simpler, cost-effective solution, then option A can work effectively.

---

### Question 67

To allow an application running on Amazon EC2 instances in Account B to access the Amazon DynamoDB table named PII in Account A, you need to configure the necessary cross-account access permissions. Here's an evaluation of each option:

#### A. Ask an Administrator in Account B to allow the EC2 IAM role permission to assume the AccessPII role:

This is a crucial step. In Account B, the EC2 IAM role should be granted permission to assume the AccessPII role in Account A. This is done by modifying the trust policy of the EC2 IAM role to include the Account A role as a trusted entity.

#### B. Ask an Administrator in Account B to allow the EC2 IAM role permission to assume the AccessPII role with predefined service control policies:

Predefined service control policies typically relate to AWS Organizations and are used to set permissions on the organization level, not for cross-account role assumption. It's not applicable in this context, and this step doesn't align with the necessary configuration for cross-account access.

#### C. Ask an Administrator in Account A to allow the EC2 IAM role permission to assume the AccessPII role with predefined service control policies:

Similar to option B, predefined service control policies don't apply to cross-account role assumption. The permissions for cross-account access are set by the trust policy in the role of the trusted entity (Account B's EC2 role).

#### D. Include the AssumeRole API in the application code logic to obtain credentials to access the PII table:

While it's technically possible to use the AssumeRole API in application code, it is not the best practice for obtaining temporary credentials for cross-account access. Using IAM roles with instance profiles is a more secure and scalable way to acquire credentials for cross-account access without embedding AssumeRole in the application code.

#### E. Include the GetSessionToken API in the application code logic to obtain credentials to access the PII table:

The GetSessionToken API is generally used to obtain temporary credentials for IAM users within the same AWS account. It's not designed for cross-account access, so this step is not appropriate for this scenario.

The correct steps to access the DynamoDB table in Account A from EC2 instances in Account B are:

#### A. Ask an Administrator in Account B to allow the EC2 IAM role permission to assume the AccessPII role.
(Assuming the trust relationship is properly configured, this allows the EC2 role to assume the role in Account A for access.)

#### D. Include the AssumeRole API in the application code logic to obtain credentials to access the PII table.
(When running code on EC2 instances, you can use the AssumeRole API to programmatically assume the role and obtain temporary credentials for accessing the DynamoDB table.)

---

### Question 68

To inspect the timing of DynamoDB API calls and identify bottlenecks in an AWS Lambda function, you can use AWS X-Ray for tracing. Let's evaluate each option:

#### A. Add DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics:

While CloudWatch metrics provide valuable insights into Lambda function performance, they don't provide detailed timing information for individual API calls within the function, such as DynamoDB API calls.

#### B. Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs:

Using an ALB is not a relevant approach for monitoring and timing DynamoDB API calls within an AWS Lambda function. ALBs are typically used for routing HTTP/HTTPS traffic, not for monitoring Lambda function performance.

#### C. Limit Lambda to no more than five concurrent invocations. Monitor from the Lambda console:

Limiting the number of concurrent Lambda invocations doesn't directly help in inspecting the timing of DynamoDB API calls. Monitoring from the Lambda console provides general information about the function's invocations but doesn't offer specific details about DynamoDB calls.

#### D. Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service:

This is the correct approach. Enabling AWS X-Ray tracing for the Lambda function allows you to capture detailed tracing information about the function's execution, including the timing of API calls to services like DynamoDB. You can view traces in the AWS X-Ray service, which helps identify bottlenecks, performance issues, and latencies in your Lambda function.

So, the most appropriate choice to inspect the timing of DynamoDB API calls in the Lambda function and improve performance is:

#### D. Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service.

---

### Question 69

To eliminate the performance impact on application users when write traffic to an Amazon RDS database affects read query performance, you should consider the following options:

#### A. Make sure Amazon RDS is Multi-AZ so it can better absorb increased traffic:

Amazon RDS Multi-AZ (Availability Zone) configuration primarily focuses on high availability and fault tolerance by maintaining a standby replica in a different Availability Zone. While it can help with availability, it may not eliminate performance impact from increased write traffic. It's not the most suitable option for addressing read query performance specifically.

#### B. Create an RDS Read Replica and direct all read traffic to the replica:

This is a good option for addressing the read query performance impact. By creating an RDS Read Replica, you offload read traffic from the primary instance to the replica, which can help maintain read query performance even when write traffic is intensive on the primary instance. This option allows you to scale read capacity independently, providing a better experience for application users.

#### C. Implement Amazon ElastiCache in front of Amazon RDS to buffer the write traffic:

Amazon ElastiCache is an in-memory caching service, and while it can help improve read performance by caching frequently accessed data, it doesn't directly address the impact of write traffic on your RDS database. It can reduce the load on the RDS instance for read-heavy workloads but doesn't eliminate the impact of write traffic.

#### D. Use Amazon DynamoDB instead of Amazon RDS to buffer the read traffic:

Amazon DynamoDB is a NoSQL database service and not directly related to read buffering for an RDS database. Switching from RDS to DynamoDB would require significant architectural changes and may not be suitable depending on your specific use case.

The most appropriate choice to eliminate the performance impact on application users in this scenario is:

#### B. Create an RDS Read Replica and direct all read traffic to the replica.

By doing this, you offload read queries to the replica, which can maintain read query performance even when write traffic is intensive on the primary RDS instance.

---

### Question 70

To meet the requirement of making coordinated, all-or-nothing changes to multiple items in the company's inventory table in Amazon DynamoDB, you should consider the following options:

#### A. Enable transactions for the DynamoDB table. Use the TransactWriteItems operation to group the changes and update the items in the table:

This is the correct and recommended solution. Amazon DynamoDB provides support for transactions, and the TransactWriteItems operation allows you to group multiple write operations into a single, atomic transaction. This ensures that either all changes succeed or none of them do, meeting the requirement of coordinated, all-or-nothing changes.

#### B. Use the BatchWriteItem operation to update the items:

While BatchWriteItem allows you to write multiple items to a DynamoDB table, it doesn't guarantee all-or-nothing changes in a transactional manner. If any item in the batch fails, the other changes are still applied, which doesn't meet the requirement of coordinated, all-or-nothing changes.

#### C. Set up a FIFO queue using Amazon SQS. Group the changes in the queue. Update the table based on the grouped changes:

Using Amazon SQS to manage changes in a queue can help with message coordination, but it doesn't inherently provide transactional behavior for updates in DynamoDB. You would need to implement additional logic to ensure all-or-nothing changes, which is more complex and less efficient than using DynamoDB's built-in transactions.

#### D. Create a transaction table in an Amazon Aurora DB cluster to manage the transactions. Write a backend process to sync the Aurora DB table and the DynamoDB table:

This approach introduces additional complexity by using both DynamoDB and Amazon Aurora, and it requires a backend process to sync the data between the two. While it's possible, it's not the most straightforward and efficient solution for handling transactions in DynamoDB.

The most suitable choice to meet the requirement of making coordinated, all-or-nothing changes to multiple items in the DynamoDB table is:

#### A. Enable transactions for the DynamoDB table. Use the TransactWriteItems operation to group the changes and update the items in the table.

---

### Question 71

To implement the requirement of storing an application metric in Amazon CloudWatch from an EC2 instance, you should consider the following options:

#### A. Use the PUT Object API call to send data to an S3 bucket. Use an event notification to invoke a Lambda function to publish data to CloudWatch:

This approach involves storing the metric data in an S3 bucket, which is not a typical method for sending metric data to CloudWatch. While you can trigger a Lambda function from S3 events, this adds unnecessary complexity for pushing metrics to CloudWatch. There are more straightforward ways to send metrics directly to CloudWatch.

#### B. Publish the metric data to an Amazon Kinesis Stream using a PutRecord API call. Subscribe to a Lambda function that publishes data to CloudWatch:

Using Amazon Kinesis is not a typical method for sending custom application metrics to CloudWatch. It's generally used for streaming and processing data in real-time. The Lambda function in this scenario introduces additional complexity that is not necessary for simply sending custom metrics to CloudWatch.

#### C. Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Provide the required credentials to enable the API call:

This is a viable option. You can use the PutMetricData API call to submit custom metrics directly to CloudWatch. You need to provide the required AWS credentials to authenticate the API call. This is a straightforward approach.

#### D. Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Launch the EC2 instance with the required IAM role to enable the API call:

This is the best practice for implementing this requirement. You can use the PutMetricData API call to send custom metrics to CloudWatch directly from the EC2 instance. To ensure secure and seamless access to CloudWatch, it's recommended to launch the EC2 instance with an IAM role that grants the necessary permissions to publish metrics to CloudWatch. This role eliminates the need to manage AWS credentials explicitly on the instance.

The most suitable and best practice for implementing this requirement is:

#### D. Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Launch the EC2 instance with the required IAM role to enable the API call.

This approach is secure, efficient, and minimizes the management overhead associated with credentials.

---

### Question 72

To design an application on AWS for consuming Amazon SQS messages ranging from 1 KB up to 1 GB in size, you need a scalable and efficient solution for message management. Let's analyze the options:

#### A. Use Amazon S3 and the Amazon SQS CLI:

This option suggests using Amazon S3, which is a valid approach for managing large messages. You could store the message payloads in S3 and then use Amazon SQS to send references (e.g., S3 object keys) to these messages. However, the Amazon SQS CLI doesn't inherently handle large message payloads well. You might need to develop additional logic for managing the messages and processing them from S3.

#### B. Use Amazon S3 and the Amazon SQS Extended Client Library for Java:

This is the most appropriate option for managing large Amazon SQS messages. The Amazon SQS Extended Client Library for Java is designed to handle larger message payloads efficiently. It stores the message payloads in Amazon S3 and uses SQS to send references to those payloads. It's a scalable and well-documented approach for working with large messages in SQS. This is especially useful for storing and consuming messages up to 2 GB. You can use the Amazon SQS Extended Client Library for Java to manage Amazon SQS messages using Amazon S3 only with the AWS SDK for Java. You can't do this with the AWS CLI, the Amazon SQS console, the Amazon SQS HTTP API, or any of the other AWS SDKs.

#### C. Use Amazon EBS and the Amazon SQS CLI:

Amazon EBS (Elastic Block Store) is not typically used for storing message payloads, and it's not the ideal solution for handling large messages in SQS. This option doesn't provide a straightforward and efficient way to manage large messages.

#### D. Use Amazon EFS and the Amazon SQS CLI:

Amazon EFS (Elastic File System) is also not commonly used for message storage in SQS. It doesn't provide a built-in mechanism for managing large messages. This is not a recommended approach for this use case.

The best solution for managing Amazon SQS messages ranging from 1 KB up to 1 GB in size is:

#### B. Use Amazon S3 and the Amazon SQS Extended Client Library for Java.

This combination efficiently handles large message payloads by storing them in S3 and using SQS to reference those payloads, making it a scalable and well-documented solution.

---

### Question 73

To efficiently fulfill the request for monitoring the number of running threads over time in a multi-threaded application running on Amazon EC2 instances, let's evaluate each option:

#### A. Periodically send the thread count to AWS X-Ray segments, then generate a service graph on demand:

AWS X-Ray is a service designed for distributed tracing and provides insights into the performance of your applications. While it's useful for tracing requests and capturing data on individual segments, it's not designed for monitoring running thread counts. Additionally, generating a service graph on demand might not be efficient for real-time monitoring of thread counts.

#### B. Create a custom Amazon CloudWatch metric and periodically perform a PutMetricData call with the current thread count:

This is the recommended and most efficient approach for monitoring the number of running threads. You can create a custom CloudWatch metric specifically for tracking the thread count, and then periodically update this metric using the PutMetricData API call from your application. CloudWatch is designed for real-time monitoring and provides graphing capabilities, making it a suitable choice for this use case.

#### C. Periodically log thread count data to Amazon S3. Use Amazon Kinesis to process the data into a graph:

While this approach allows you to store the thread count data, it involves more complexity than necessary for monitoring the thread count. Using S3 and Kinesis introduces additional steps and services that may not be required for this task.

#### D. Periodically write the current thread count to a table using Amazon DynamoDB and use Amazon CloudFront to create a graph:

Amazon DynamoDB is a NoSQL database and is not optimized for time-series data or graphing. Using Amazon CloudFront doesn't make sense in this context, as CloudFront is a content delivery network service, not a graphing tool. This approach is overly complex and not well-suited for monitoring thread counts.

The most efficient and straightforward way to fulfill the request for monitoring the number of running threads over time is:

#### B. Create a custom Amazon CloudWatch metric and periodically perform a PutMetricData call with the current thread count.

This approach is designed for real-time monitoring and provides an efficient and straightforward solution for tracking and graphing the thread count.

---

### Question 74

To ensure compliance with the security policy of storing sensitive audio and video files in an Amazon S3 bucket, with the requirement that all data written to the bucket must be encrypted, let's examine each option:

#### A. Use AWS Lambda to send notifications to the security team if unencrypted objects are put in the bucket:

This approach would help identify and notify the security team when unencrypted objects are put in the S3 bucket. However, it doesn't directly prevent the upload of unencrypted objects. It is a monitoring solution, not a prevention mechanism.

#### B. Configure an Amazon S3 bucket policy to prevent the upload of objects that do not contain the x-amz-server-side-encryption header:

This option is a valid approach to enforce the requirement of encrypting all objects stored in the S3 bucket. By configuring a bucket policy that denies uploads of objects lacking server-side encryption headers, you ensure that only encrypted data can be stored in the bucket, thereby complying with the security policy. To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.

#### C. Create an Amazon CloudWatch event rule to verify that all objects stored in the Amazon S3 bucket are encrypted:

While CloudWatch can be used for monitoring and event-driven actions, it doesn't inherently enforce encryption requirements. It can notify or trigger actions based on events, but it doesn't prevent unencrypted uploads.

#### D. Configure an Amazon S3 bucket policy to prevent the upload of objects that contain the x-amz-server-side-encryption header:

This option is not a recommended approach. Configuring a bucket policy to prevent uploads of objects with server-side encryption headers would effectively block all uploads because the x-amz-server-side-encryption header is typically included in requests when server-side encryption is enabled. This would prevent legitimate uploads.

The most suitable option for ensuring compliance with the policy that requires all data written to the S3 bucket to be encrypted is:

#### B. Configure an Amazon S3 bucket policy to prevent the upload of objects that do not contain the x-amz-server-side-encryption header.

This policy ensures that unencrypted objects cannot be uploaded to the bucket, effectively enforcing the encryption requirement.

---

### Question 75

To solve the issue where access is denied when a user attempts to access the assets bucket from the code bucket when using Amazon S3 for static website hosting, you need to ensure that the assets bucket is accessible by the code bucket. Let's examine each option:

#### A. Create an IAM role and apply it to the assets bucket for the code bucket to be granted access:

IAM roles are used to grant temporary permissions for AWS services to access resources. However, using IAM roles for S3 buckets in this context is not suitable for static website hosting, and it's not a recommended approach. IAM roles are typically used for server-side operations, not for allowing access between S3 buckets.

#### B. Edit the bucket policy of the assets bucket to open access to all principals:

Editing the bucket policy of the assets bucket to open access to all principals is a solution but may not be the most secure choice. It would allow anyone with permissions to the code bucket to access the assets bucket, which might not align with security best practices.

#### C. Edit the cross-origin resource sharing (CORS) configuration of the assets bucket to allow any origin to access the assets:

CORS rules specify which domains are allowed to access the resources of a web page on your domain. This option is not suitable for granting access between S3 buckets, and it doesn't address the problem of access from the code bucket to the assets bucket.

#### D. Change the code bucket to use AWS Lambda functions instead of static website hosting:

Changing the code bucket to use AWS Lambda functions instead of static website hosting is not a direct solution to the issue at hand. This approach introduces significant changes and complexity that may not be necessary for resolving the access issue.

The most straightforward and secure solution to resolve this issue is to:

#### B. Edit the bucket policy of the assets bucket to open access to all principals.

This option allows you to specify the permissions in a controlled manner, ensuring that only the code bucket (or specific principals) can access the assets bucket, while still granting the necessary access. Be careful to define permissions carefully to maintain security.

---

### Question 76

Let's break down each option for the situation where the S3 bucket's size increased significantly from 100 KB to 50 GB after moving CSS files to a folder:

#### A. The CSS files were not compressed, and S3 versioning was enabled:

This option suggests that the CSS files were not compressed, and S3 versioning was enabled. While uncompressed files may contribute to storage usage, enabling S3 versioning alone does not directly cause a significant increase in storage. Versioning retains multiple versions of the same object, but it doesn't inherently lead to a rapid growth in the bucket's size. So, this option is less likely to be the cause of the issue.

#### B. S3 replication was enabled on the bucket:

Enabling S3 replication allows data to be copied to another S3 bucket, but it doesn't directly cause data to grow in the source bucket. If replication was enabled, it would result in data being copied to another location, but it wouldn't explain why the source bucket's size increased significantly.

#### C. Logging into the same bucket caused exponential log growth:

This is the most likely cause. When server access logging is enabled for an S3 bucket, it generates log records for each access to the bucket's objects. These log records are typically stored in the same bucket or another specified location. When objects are moved to different folders within the same bucket, the movement itself generates additional access events, which are logged and stored. Over time, as more access events occur, the log records accumulate, leading to increased storage usage.
Don't push server access logs about a bucket into the same bucket. If you configured your server access logs this way, then there would be an infinite loop of logs. This is because when you write a log file to a bucket, the bucket is also accessed, which then generates another log. A log file would be generated for every log written to the bucket, which creates a loop. This would create many logs and increase your storage costs.

#### D. An S3 lifecycle policy has moved the entire CSS file to S3 Infrequent Access:

An S3 lifecycle policy can transition objects to different storage classes like S3 Infrequent Access based on certain criteria. While this might lead to changes in storage costs, it typically wouldn't cause a significant and rapid increase in the bucket's size. It might save costs by moving data to a cheaper storage class, but it wouldn't explain the observed size increase.

In summary, option C, "Logging into the same bucket caused exponential log growth," is the most likely cause of the significant size increase in the S3 bucket. Enabling server access logging and performing actions within the same bucket can lead to the generation of numerous log records, which contribute to the increased storage size.

---

### Question 77

Let's go through each option for supporting authentication for the developer's dashboard application:

#### A. AWS KMS (AWS Key Management Service):

AWS Key Management Service is primarily a service for managing encryption keys and is not intended for user authentication. It helps you create and control encryption keys to protect your data, but it doesn't provide the user management and authentication features required for a sign-in mechanism.

#### B. Amazon Cognito:

Amazon Cognito is a fully managed service that provides user identity and access control to secure your applications. It's designed specifically for scenarios like the one described. With Amazon Cognito, you can easily set up user sign-in and registration, support various identity providers (e.g., social identity providers like Google and Facebook), and synchronize user data across devices and platforms. It's suitable for mobile applications, desktops, and tablets, and it can remember user preferences across these devices. This is the most appropriate choice for the developer's authentication scenario.

#### C. AWS Directory Service:

AWS Directory Service is used for setting up and managing directory services for Microsoft AD and LDAP. It's not designed for user authentication in a general application context and doesn't provide the necessary features for this use case.

#### D. Amazon IAM (Identity and Access Management):

Amazon IAM is primarily used for managing access to AWS resources. It's not a user authentication service for application users, and it doesn't provide features for creating user identities, registration, and authentication for the application's users.

The best choice for supporting user authentication, particularly for a diverse set of devices and platforms like mobile applications, desktops, and tablets, and ensuring user preferences synchronization, is:

#### B. Amazon Cognito.

---

### Question 78

To securely authenticate a CloudWatch PUT request when instances in an Auto Scaling group need to publish custom metrics, you should consider the following options:

#### A. Create an IAM user with PutMetricData permission and put the user credentials in a private repository; have applications pull the credentials as needed.

This option is not recommended for security reasons. Storing IAM user credentials in a private repository that applications pull from is not a best practice and can lead to security risks.

#### B. Create an IAM user with PutMetricData permission, and modify the Auto Scaling launch configuration to inject the user credentials into the instance user data.

This approach is better than option A as it allows you to inject IAM user credentials into the instance user data. However, it's still not the most secure way to authenticate PUT requests to CloudWatch.

#### C. Modify the CloudWatch metric policies to allow the PutMetricData permission to instances from the Auto Scaling group.

This option seems to suggest modifying the CloudWatch policies to allow instances from the Auto Scaling group to send custom metrics directly. While this could work, it doesn't specify how the instances themselves authenticate. Moreover, modifying CloudWatch policies directly may lead to potential security issues.

#### D. Create an IAM role with PutMetricData permission and modify the Auto Scaling launching configuration to launch instances using that role.

This is the recommended approach for securely authenticating CloudWatch PUT requests from instances within an Auto Scaling group. By creating an IAM role with the necessary permissions and attaching it to the Auto Scaling group's launch configuration, instances will automatically be granted temporary credentials with the appropriate permissions to publish custom metrics to CloudWatch. This is the most secure and AWS best practice method for authentication.

So, the MOST secure way to authenticate a CloudWatch PUT request from instances in an Auto Scaling group is:

#### D. Create an IAM role with PutMetricData permission and modify the Auto Scaling launching configuration to launch instances using that role.

---

### Question 80

To monitor API errors in the code and receive notifications if failures go above a set threshold value in an AWS environment, you should consider the following options:

#### A. Publish a custom metric on Amazon CloudWatch and use Amazon SES for notification:

This option suggests publishing a custom metric for monitoring API errors in Amazon CloudWatch, which allows you to collect and visualize custom application data. It also proposes using Amazon Simple Email Service (SES) for notifications. While CloudWatch can help you collect metrics, SES is typically used for sending emails and might not be the best choice for real-time notifications. SES is more suitable for sending email alerts, but it's not a real-time notification service like Amazon SNS.

#### B. Use an Amazon CloudWatch API-error metric and use Amazon SNS for notification:

This option suggests using Amazon CloudWatch to collect API-error metrics, which is a more straightforward and direct approach to monitor API errors. It then recommends using Amazon Simple Notification Service (SNS) for notifications. Amazon SNS is a highly scalable and reliable service designed for real-time, event-driven notifications. This option aligns well with best practices for monitoring and notification.

#### C. Use an Amazon CloudWatch API-error metric and use Amazon SES for notification:

Similar to option A, this option involves using CloudWatch to collect API-error metrics. However, it suggests using Amazon SES for notifications, which, as mentioned earlier, is not the most suitable choice for real-time notifications.

#### D. Publish a custom metric on Amazon CloudWatch and use Amazon SNS for notification:

This option combines publishing a custom metric to CloudWatch for monitoring with Amazon SNS for notifications, which is a valid and commonly used approach for monitoring and notification in AWS. Amazon SNS is well-suited for real-time notifications, making it a good choice. Third Party API -> Custom Metric

Ultimately, the choice between option B (using Amazon CloudWatch API-error metric with SNS) and option D (using custom metrics with SNS) depends on the level of customization and specificity required for monitoring your third-party API. Option B, with built-in CloudWatch metrics, is often simpler and more straightforward, making it a preferred choice for many common scenarios. However, if your monitoring needs are unique, option D might be more suitable.

---

### Question 81

In AWS CodePipeline, you can implement manual approval steps as part of your release process workflow. Let's go through each of the options and provide detailed explanations for each:

#### A. Use multiple pipelines to allow approval:

This option involves setting up multiple pipelines, where each pipeline represents a different stage in your application's release process. For example, you might have one pipeline for development, one for testing, and one for production. You can set up manual approvals at the transition points between these pipelines. When a pipeline completes its tasks, it can trigger the next pipeline. Before the code is deployed to production, you can configure a manual approval step. This allows you to ensure that code doesn't progress to the production pipeline without manual approval. While this method can work, it's not the most efficient approach and can be complex to manage as your pipeline stages increase.

#### B. Use an approval action in a stage:

This is the recommended and the best way to achieve manual approval in AWS CodePipeline. You can add an "Approval" action in a specific stage of your pipeline. This action allows you to specify who should approve the deployment and configure notification settings. When the pipeline reaches the stage with the approval action, it will pause and wait for manual approval before proceeding further. Once the designated approver grants approval, the pipeline will continue to the next stage. This approach is straightforward and provides fine-grained control over when and where approvals are required. You can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.

#### C. Disable the stage transition to allow manual approval:

Disabling a stage transition means preventing the automatic progression of the pipeline to the next stage. This would require manual intervention to allow the pipeline to continue. While this can be used to achieve manual approval, it's not the best practice. AWS provides a dedicated "Approval" action for this purpose, which is more intuitive and offers better visibility into the approval process. Disabling a stage transition is not as straightforward and can lead to confusion in pipeline management.

#### D. Disable a stage just prior to the deployment stage:

Disabling a stage just before the deployment stage is not a recommended way to achieve manual approval. It can introduce complexities and potential risks to your pipeline management. If you need manual approval, it's better to use an "Approval" action specifically designed for this purpose. Disabling a stage in this manner may lead to confusion, and it's not as transparent and organized as using an approval action.

In summary, the BEST way to achieve manual approval in AWS CodePipeline is option B: Use an approval action in a stage. This method is specifically designed for manual approval, is straightforward to set up, and provides a clear and organized approval process within your pipeline.

---

### Question 82

To implement a caching layer in front of Amazon RDS while maintaining maximum uptime and considering the expensive regeneration of cached content in case of service failure, let's examine each option in detail:

#### A. Implement Amazon ElastiCache Redis in Cluster Mode:

Amazon ElastiCache with Redis is a popular choice for caching due to its high performance and advanced features. Implementing Redis in Cluster Mode provides both data partitioning and high availability. Redis Cluster can be set up with multiple nodes, and it automatically handles data distribution and failover. If one node fails, the data can still be accessed from the remaining nodes, ensuring maximum uptime. This option is a suitable choice for maintaining high availability while benefiting from Redis caching capabilities.

#### B. Install Redis on an Amazon EC2 instance:

While you can install Redis on an Amazon EC2 instance, this approach doesn't provide the same level of high availability as Amazon ElastiCache with Redis in Cluster Mode. EC2 instances can fail, and if the instance hosting Redis goes down, it may result in service interruptions. To achieve high availability, you would need to set up replication and failover mechanisms manually, which can be complex and may not be as reliable as the managed services offered by Amazon ElastiCache.

#### C. Implement Amazon ElastiCache Memcached:

Amazon ElastiCache also supports Memcached, which is another popular caching solution. While Memcached can provide high performance, it doesn't inherently provide data persistence and advanced features like data partitioning and high availability as Redis does. If maintaining maximum uptime is critical and the data is expensive to regenerate, Redis in Cluster Mode (Option A) might be a more suitable choice compared to Memcached.

#### D. Migrate the database to Amazon Redshift:

Amazon Redshift is a data warehousing service and is not intended for caching purposes. Migrating your database to Redshift would not provide caching capabilities and is not a suitable solution for this use case.

In summary, the BEST option for implementing a caching layer in front of Amazon RDS while maintaining maximum uptime and considering the expensive regeneration of cached content is Option A: Implement Amazon ElastiCache Redis in Cluster Mode. This option provides both data partitioning and high availability, ensuring that cached data remains accessible even in the event of a service failure.

---

### Question 83

To add thumbnail generation to the existing serverless application while meeting user requirements and minimizing changes to the existing code, let's examine each option in detail:

#### A. Change the existing Lambda function handling the uploads to create thumbnails at the time of upload. Have the function store both the image and thumbnail in Amazon S3:

While this option is straightforward, it may impact the time it takes to perform image uploads since thumbnail generation can be a time-consuming process, especially if it's done synchronously within the same Lambda function. This could potentially slow down the image processing. Also, it doesn't follow best practices for separation of concerns in a serverless architecture.

#### B. Create a second Lambda function that handles thumbnail generation and storage. Change the existing Lambda function to invoke it asynchronously:

This option is a better approach. By creating a separate Lambda function for thumbnail generation and invoking it asynchronously, you avoid impacting the existing image uploads. The original Lambda function can continue to handle image uploads without delay. This approach also adheres to serverless best practices by keeping functions focused on single tasks. "You have to make sure upload to s3 is completed."

#### C. Create an S3 event notification with a Lambda function destination. Create a new Lambda function to generate and store thumbnails:

This option is a good choice as well. You can configure an S3 event notification to trigger a Lambda function when a new image is uploaded. This new Lambda function can then generate and store thumbnails without impacting the original image upload process. This is an efficient way to achieve thumbnail generation in a serverless manner while keeping changes to the existing code minimal.

#### D. Create an S3 event notification to an SQS Queue. Create a scheduled Lambda function that processes the queue, generates and stores thumbnails:

This option introduces complexity with the use of an SQS queue and a scheduled Lambda function. It can work, but it's not the most straightforward approach, and it may introduce unnecessary latency due to the scheduled processing. Options B and C are more suitable for this use case.

In summary, the BEST options for adding thumbnail generation to the existing serverless application while minimizing changes to the existing code and avoiding any impact on image upload times are options B and C. These options allow you to efficiently generate and store thumbnails without disrupting the existing functionality.

---

### Question 84

In the given scenario, where a Java AWS Lambda function is used to process images upon upload to an Amazon S3 bucket and copy the resulting images to another bucket, let's provide detailed explanations for each option:

#### A. The Lambda function has insufficient memory and needs to be increased to 1 GB to match the Amazon EC2 instance:

This option suggests that the Lambda function may not have enough memory allocated to it, which can affect its performance. Increasing the memory allocation to 1 GB can potentially provide more CPU power and network bandwidth for faster execution. It's based on the assumption that the function's slow execution may be due to insufficient resources. However, this is not a definitive solution, and the issue could be more complex than just memory allocation.

#### B. Files need to be copied to the same Amazon S3 bucket for processing, so the second bucket needs to be deleted:

This option suggests that processing and storage should occur in the same S3 bucket. However, the provided information in the question doesn't explicitly indicate that, and it doesn't address the primary issue of why no images are being copied to the second S3 bucket. This option is unrelated to the actual problem of images not being copied.

#### C. Lambda functions have a maximum execution limit of 300 seconds, therefore the function is not completing:

This option is incorrect based on the updated information that Lambda functions have a maximum execution limit of 900 seconds, which exceeds the function's execution time of 500 seconds. The execution time limit is not the primary issue in this case. (Old question, the execution time limit was 300 seconds, it was increased to 900 seconds late 2018, answer would have been C)

#### D. There is a problem with the Java runtime for Lambda, and the function needs to be converted to node.js:

This option suggests that the Java runtime for Lambda may be causing the problem and proposes converting the function to Node.js as a solution. However, the choice of runtime may not be the primary issue, and the conversion to Node.js may not be necessary to solve the problem. The root cause may lie elsewhere in the code or configuration.

In conclusion, while option A suggests a possible solution related to memory allocation, it is important to note that the actual problem could be more complex, and increasing memory may not be the only solution. The provided information in the question doesn't definitively point to a single cause, and additional investigation and debugging may be required to determine the root cause of why images are not being copied to the second S3 bucket.

---

### Question 85

To implement encryption at rest for data within Amazon Kinesis Streams with a use case where clickstream data may not be consumed for up to 12 hours, let's examine each option:

#### A. Enable SSL connections to Kinesis:

Enabling SSL (Secure Sockets Layer) connections to Kinesis Streams provides encryption in transit, which secures data while it's being transmitted over the network. However, this option does not address data at rest. To encrypt data at rest, you need to focus on server-side encryption or other mechanisms.

#### B. Use Amazon Kinesis Consumer Library:

The Amazon Kinesis Consumer Library is a client library for processing data from Kinesis Streams. It does not directly address encryption at rest. It's primarily designed to help applications read and process data from the stream efficiently.

#### C. Encrypt the data once it is at rest with a Lambda function:

This option suggests using a Lambda function to encrypt data after it is already at rest. While this approach can work, it doesn't directly address data encryption within Kinesis Streams itself. Server-side encryption is a more direct way to achieve encryption at rest.

#### D. Enable server-side encryption in Kinesis Streams:

This is the correct option for implementing encryption at rest for data within Amazon Kinesis Streams. By enabling server-side encryption, data is automatically encrypted when it's written to the stream and decrypted when it's read. Kinesis supports server-side encryption with AWS Key Management Service (KMS) and can help protect data at rest for up to 12 hours or longer.

In summary, the BEST way to implement encryption at rest for data within Amazon Kinesis Streams is option D: Enable server-side encryption in Kinesis Streams. This ensures that data is automatically encrypted and decrypted as it is written to and read from the stream, providing data security at rest.

---

### Question 86

To meet the requirements of creating a mobile application with customer sign-up and authentication using the organization's current SAML 2.0 identity provider, let's provide detailed explanations for each option:

#### A. AWS Lambda: AWS Lambda is a serverless compute service that allows you to run code in response to various events or triggers. While Lambda can be used to build serverless applications and perform specific tasks, it is not the best choice for implementing user authentication and sign-up flows. Lambda alone does not provide built-in features for user identity management, sign-up, or authentication.

#### B. Amazon Cognito: Amazon Cognito is a fully managed identity and user management service provided by AWS, and it is the best choice for the described use case. Amazon Cognito supports user sign-up and sign-in, and it integrates seamlessly with various identity providers, including SAML 2.0 identity providers. It is designed to handle user identity, authentication, and authorization for mobile and web applications, making it a cost-effective solution for creating scalable and secure user authentication flows with minimal budget and development effort.

#### C. AWS IAM (Identity and Access Management): AWS IAM is a service for managing AWS resources, permissions, and users within the AWS ecosystem. It is not intended for mobile application user management or authentication. While it's an essential part of securing AWS resources, it is not designed for creating user authentication flows for external mobile applications.

#### D. Amazon EC2 (Elastic Compute Cloud): Amazon EC2 provides scalable compute capacity, but it is a virtual server infrastructure and not a service for user authentication or identity management. Using EC2 to implement user authentication and sign-up would be an overcomplicated and costly approach for this use case.

In summary, the BEST AWS service to meet the requirements of creating a mobile application with user sign-up and authentication using the organization's SAML 2.0 identity provider on a limited budget is option B: Amazon Cognito. Amazon Cognito is specifically designed for user identity management and authentication and offers seamless integration with identity providers, making it the most suitable and cost-effective choice for this use case.

---

### Question 87

To implement autoscaling based on the number of concurrent users for a web application in AWS, let's provide detailed explanations for each option:

#### A. An Amazon SNS topic to be triggered when a concurrent user threshold is met:

Using Amazon SNS (Simple Notification Service) to send notifications when a concurrent user threshold is met is not a direct way to implement autoscaling. SNS is a messaging service used for publishing and subscribing to topics, and it can be used for notifying various services or users when a specific event occurs. However, it doesn't directly facilitate autoscaling based on concurrent users.

#### B. An Amazon CloudWatch Networking metric:

While Amazon CloudWatch can monitor various AWS resources and services, it does not inherently provide metrics for concurrent users out-of-the-box. Network metrics might monitor network-related data, but they are not a direct measurement of user activity or concurrent users. Using CloudWatch Networking metrics would not be suitable for autoscaling based on concurrent users.

#### C. Amazon CloudFront to leverage AWS Edge Locations:

Amazon CloudFront is a content delivery service that uses AWS Edge Locations to cache and distribute content closer to end users. It is not directly related to autoscaling based on concurrent users. While it can help improve content delivery and reduce latency, it doesn't provide the mechanism for autoscaling based on user concurrency.

#### D. A Custom Amazon CloudWatch metric for concurrent users:

This is the most appropriate option. To autoscale based on concurrent users, you would need to implement a custom CloudWatch metric that measures the number of concurrent users. You can instrument your web application to send data to CloudWatch, and then use CloudWatch alarms to trigger autoscaling events based on the custom metric's threshold. This approach allows you to directly measure and respond to concurrent user activity and scale resources accordingly. There is no 'concurrent user threshold' metric for Auto Scaling. You should create custom metric, if you want to scale depending on this value.

In summary, the BEST option for autoscaling based on the number of concurrent users for a web application in AWS is option D: A Custom Amazon CloudWatch metric for concurrent users. This approach allows you to measure user concurrency directly and configure autoscaling events based on your custom metric's threshold.

---

### Question 88

To automate the deployment of a serverless application with the ability to rollback based on the provided requirements, let's provide detailed explanations for each option:

#### A. Deploy using Amazon Lambda API operations to create the Lambda function by providing a deployment package:

While you can use the AWS Lambda API to create and manage Lambda functions, it does not provide a comprehensive deployment solution for a serverless application with multiple AWS services and dependencies. It lacks the ability to define the entire application infrastructure and configuration.

#### B. Use an AWS CloudFormation template and use CloudFormation syntax to define the Lambda function resource in the template:

This option is a viable choice. AWS CloudFormation is a powerful service that allows you to define your entire AWS infrastructure, including Lambda functions, in a template. You can use CloudFormation to define the Lambda function, its dependencies, API Gateway configuration, and DynamoDB tables, all in one template. This allows you to create, update, and potentially rollback the entire application as a single unit.

#### C. Use syntax conforming to the Serverless Application Model in the AWS CloudFormation template to define the Lambda function resource:

This is also a valid approach. The AWS Serverless Application Model (SAM) is an extension of CloudFormation designed for serverless applications. It provides a simplified way to define serverless resources, including Lambda functions. SAM templates can be used to define the Lambda function and other related resources, making it easier to manage serverless applications.

#### D. Create a bash script that uses AWS CLI to package and deploy the application:

While it's possible to create deployment scripts using the AWS CLI, this approach is typically less structured and harder to manage, especially for complex serverless applications with multiple dependencies. It might not provide an easy way to perform rollbacks or manage the entire application infrastructure.

In summary, the BEST options for automating the deployment of a serverless application with the ability to rollback are options B and C. Using an AWS CloudFormation template with either standard CloudFormation syntax or the Serverless Application Model (SAM) allows you to define the entire application's infrastructure and dependencies, making it easier to manage deployments and rollbacks.

---

### Question 89

To ensure that individual users do not have access to other users' game data in an Amazon DynamoDB table, let's provide detailed explanations for each option:

#### A. Encrypt the game data with individual user keys:

Encrypting game data with individual user keys can provide data security and privacy. However, this option doesn't directly address access control in DynamoDB. It's more about data encryption at rest or in transit. While encryption is a good security practice, it doesn't prevent unauthorized access to data in DynamoDB. Therefore, this option alone is not sufficient to restrict access to specific users' data.

#### B. Restrict access to specific items based on certain primary key values:

This is the correct option for controlling access to individual users' game data in DynamoDB. You can use fine-grained access control with IAM (Identity and Access Management) or Amazon Cognito identity pools to restrict access to specific items or rows based on the primary key values. Each user's data can be uniquely identified in the DynamoDB table, and permissions can be set up to allow users to read and write only their own data while denying access to other users' data.

#### C. Stage data in SQS queues to inject metadata before accessing DynamoDB:

Using SQS queues to stage data can be a part of an application architecture, but it doesn't directly address the access control requirement. While metadata injection may help in certain scenarios, it doesn't inherently ensure that users cannot access other users' game data in DynamoDB.

#### D. Read records from DynamoDB and discard irrelevant data client-side:

This option suggests that the client-side application should filter out irrelevant data. However, it doesn't provide strong access control. Users could still attempt to read other users' data, and client-side filtering is not a secure access control mechanism. Access control should be implemented server-side or through IAM/Cognito on AWS services.

In summary, the BEST option for ensuring that individual users do not have access to other users' game data in an Amazon DynamoDB table is option B: Restrict access to specific items based on certain primary key values. This option allows for fine-grained access control and ensures that each user can only access their own data within the DynamoDB table.

---

### Question 90

To build an authentication and authorization model for APIs served through Amazon API Gateway, while allowing access based on OpenID identity providers such as Amazon or Facebook, let's provide detailed explanations for each option:

#### A. Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens:

This is the simplest and most secure design among the options provided. Amazon Cognito user pools are designed for user management, authentication, and authorization. They integrate seamlessly with OpenID identity providers and can issue JSON Web Tokens (JWTs) for authentication. You can create a custom authorizer in API Gateway to validate JWTs and authorize users based on a custom authorization model. This option follows best practices for security, is easy to set up, and offers flexibility in defining your authorization logic.

#### B. Build an OpenID token broker with Amazon and Facebook. Users will authenticate with these identity providers and pass the JSON Web Token to the API to authenticate each API call:

Building a custom token broker adds complexity and maintenance overhead. It requires you to create and maintain a custom solution for handling token authentication, and it may not provide the same level of security and ease of use as Amazon Cognito. Handling and validating tokens in a custom broker can introduce security risks and operational complexities.

#### C. Store user credentials in Amazon DynamoDB and have the application retrieve temporary credentials from AWS STS. Make API calls by passing user credentials to the APIs for authentication and authorization:

Storing user credentials in Amazon DynamoDB is not recommended for security reasons, as it can introduce risks associated with storing sensitive information. Passing user credentials to APIs is also not a secure practice and should be avoided. The use of temporary credentials from AWS Security Token Service (STS) is typically used for cross-service AWS access and is not suitable for user authentication and authorization.

#### D. Use Amazon RDS to store user credentials and pass them to the APIs for authentication and authorization:

Storing user credentials in Amazon RDS is more suitable for a traditional authentication system and doesn't integrate well with OpenID identity providers. Passing user credentials to APIs is not a secure practice and can expose sensitive information. This option is not aligned with best practices for API security.

In summary, the BEST and simplest design for building an authentication and authorization model for APIs, while allowing access based on OpenID identity providers, is option A: Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens. This approach offers security, ease of implementation, and seamless integration with OpenID identity providers.

---

### Question 91

To support a web application that requires both authentication and guest access, let's provide detailed explanations for each option:

#### A. IAM temporary credentials using AWS STS:

AWS Identity and Access Management (IAM) temporary credentials are typically used for granting access to AWS services and resources within your AWS account. They are not suitable for allowing guest access to a web application because they are designed for managing access within your AWS environment and not for external users or guest access.

#### B. Amazon Directory Service:

Amazon Directory Service provides directory solutions that are typically used for managing identities and authentication within an organization. It is not designed for providing guest access to external users or web applications. This option is not suitable for the scenario described.

#### C. Amazon Cognito with unauthenticated access enabled:

This is the correct option for allowing both authenticated users and guests to access a web application. Amazon Cognito is a service specifically designed for user authentication and identity management. With Amazon Cognito, you can enable unauthenticated access, which allows guest users to access certain parts of your application without the need to authenticate. Authenticated users can sign in, while guest users can have limited access without authentication.

#### D. IAM with SAML integration:

IAM with Security Assertion Markup Language (SAML) integration is a mechanism for federated identity and single sign-on (SSO) in AWS. It is not designed for allowing guest access to web applications. SAML integration is typically used to enable users from one organization to access resources in another organization, but it doesn't address the guest access requirement.

In summary, the BEST service for providing support for a web application to allow guest access while also supporting authentication is option C: Amazon Cognito with unauthenticated access enabled. Amazon Cognito allows you to manage both authenticated and guest users, making it a suitable choice for the described scenario.

---

### Question 92

To address the deployment error where the Lambda function size exceeds the AWS Lambda limit, let's provide detailed explanations for each option:

#### A. Submit a limit increase request to AWS Support to increase the function to the size needed:

This option involves requesting an increase in the Lambda function size limit from AWS Support. While it's possible to request such increases, it's typically not the best practice to rely on limit increases to solve this issue. Lambda functions should ideally be designed to fit within the existing limits.

#### B. Use a compression algorithm that is more efficient than ZIP:

While improving the compression algorithm may reduce the size of the deployment package, the key issue here is that the Lambda function is already too large for the current limit. A more efficient compression algorithm may help, but it may not bring the function's size within the limit.

#### C. Break the function into multiple smaller Lambda functions:

This is a common and effective approach. If the Lambda function is too large, you can modularize it by breaking it into multiple smaller Lambda functions. Each function can then be deployed separately, and you can coordinate them as needed using services like AWS Step Functions or by directly invoking one Lambda function from another.

#### D. ZIP the ZIP file twice to compress it further:

Compressing a ZIP file twice is not a valid or recommended solution. ZIP files are typically not designed to be compressed further by double-zipping. Additionally, the problem here is not typically about compression efficiency but about the Lambda function exceeding the size limit.

In summary, the BEST option for addressing the problem of a Lambda function exceeding the AWS Lambda size limit is option C: Break the function into multiple smaller Lambda functions. This approach allows you to stay within the Lambda limits while organizing and modularizing your application's functionality.

---

### Question 93

To store session information across function calls in a serverless application using API Gateway and AWS Lambda, let's provide detailed explanations for each option:

#### A. In an Amazon DynamoDB table:

This is a valid and recommended approach. Storing session information in an Amazon DynamoDB table allows you to maintain session state across Lambda function invocations. DynamoDB is a scalable, highly available, and fully managed NoSQL database service that can store data in a way that is easily accessible from multiple Lambda function executions. It provides durability and the ability to scale, making it a suitable choice for session storage.

#### B. In an Amazon SQS queue:

Storing session information in an Amazon SQS (Simple Queue Service) queue is not a typical or recommended approach for session management. SQS is primarily designed for reliable and scalable messaging between distributed systems and is not meant for maintaining session state. Using SQS for session storage could introduce unnecessary complexity and inefficiency.

#### C. In the local filesystem:

Storing session information in the local filesystem is not a recommended approach for serverless applications. Lambda functions are stateless, and the local filesystem is not shared between function invocations. Session data stored locally would be lost as soon as the function execution ends. It is not a suitable option for maintaining session state.

#### D. In an SQLite session table using CDSQLITE_ENABLE_SESSION:

AWS Lambda does not support the use of a local SQLite database with session state, and the option provided, CDSQLITE_ENABLE_SESSION, is not a recognized feature in AWS Lambda. Storing session information in a local database within the Lambda function is not a recommended approach for session management, as Lambda functions are stateless and do not persist data between invocations.

In summary, the BEST option for storing session information across function calls in a serverless application using API Gateway and AWS Lambda is option A: In an Amazon DynamoDB table. DynamoDB is designed for scalability and durability and is well-suited for managing session state in a serverless environment.

---

### Question 94

To handle ProvisionedThroughputExceeded errors in an application that reads data from an Amazon DynamoDB table, let's provide detailed explanations for each option:

#### A. Create a new global secondary index for the table to help with the additional requests:

Creating a new global secondary index (GSI) does not directly address the issue of ProvisionedThroughputExceeded errors. GSIs are used to enhance query flexibility and performance but do not affect the provisioned throughput capacity of the table itself. This option is not a suitable solution for mitigating throughput exceeded errors.

#### B. Retry the failed read requests with exponential backoff:

This is the recommended approach to handle ProvisionedThroughputExceeded errors. When you receive these errors, it indicates that the table's provisioned read capacity is momentarily exceeded. By implementing exponential backoff, you allow your application to retry the requests with increasing time intervals between retries. This approach helps prevent overloading the table and can lead to a successful read once the capacity becomes available.

#### C. Immediately retry the failed read requests:

While immediate retries may be effective in some cases, they can exacerbate the problem by repeatedly overloading the table, causing more ProvisionedThroughputExceeded errors. Using exponential backoff is generally a better practice to mitigate these errors without causing additional stress on the table.

#### D. Use the DynamoDB 'UpdateItem' API to increase the provisioned throughput capacity of the table:

This option allows you to increase the provisioned throughput capacity of the table. While it can be a valid approach, it may not be the most cost-effective one, as it would involve ongoing increased costs when higher throughput is not consistently needed. It's better to use this option after you have assessed your capacity requirements and determined that an increase in provisioned throughput is necessary for your application's regular load. Should only increase capacity if the errors are persistent, not occasional like the case of the question. The key here is "15 second" this is a short lived solution that retry can fix.

In summary, the BEST approach to handle ProvisionedThroughputExceeded errors is option B: Retry the failed read requests with exponential backoff. This method helps manage errors and retries in a way that prevents overloading the table while efficiently utilizing provisioned throughput capacity.

---

### Question 95

To maintain full capacity during updates while minimizing cost for a Linux-based application running on AWS Elastic Beanstalk, let's provide detailed explanations for each option:

#### A. Immutable: The Immutable deployment policy creates a new set of Amazon EC2 instances with the updated application version and then swaps the environment to use the new instances. It allows you to maintain full capacity during updates by ensuring that the old instances continue to serve traffic until the new instances are ready. This minimizes downtime but may result in additional costs due to temporarily running both the old and new instances concurrently. Assume you have 4 instances running with this configuration, Immutable creates 4 more instances for updated version then swaps these with old ones if successful (additional cost from 4 new instances, no donwtime).

#### B. Rolling: The Rolling deployment policy updates instances in batches, stopping and replacing them with instances running the new application version. It provides a balance between maintaining capacity and cost control. However, it may result in reduced capacity during the update process as instances are replaced, potentially affecting application performance during the rollout. It can be called as rolling without additional batches (no extra cost but downtime introduced).

#### C. All at Once: The All at Once deployment policy updates all instances simultaneously. This approach minimizes the cost associated with running both old and new instances concurrently. However, it may cause a significant and temporary drop in capacity, potentially leading to application downtime during the update. It does not meet the requirement of maintaining full capacity during updates. No extra cost but downtime introduced.

#### D. Rolling with additional batch: The Rolling with additional batch deployment policy is similar to the Rolling policy but adds a new batch of instances to replace the old instances. This helps maintain a higher level of capacity during updates, reducing the risk of downtime. While it may result in temporary cost increases due to running both old and new instances, it provides a good balance between maintaining capacity and minimizing costs. You will have 1 batch at a time for update then swap it with one of the old one, step by step (1 additional instance cost at a time, no downtime).

In this scenario, to maintain full capacity during updates while minimizing cost, the BEST deployment policy to specify for the Elastic Beanstalk environment is option D: Rolling with additional batch. This policy helps ensure that there is minimal downtime and maintains a higher level of capacity during updates while being cost-effective.

---

### Question 96

When writing a Lambda function, instantiating AWS clients outside the scope of the handler offers several benefits. Let's provide detailed explanations for each option:

#### A. Legibility and stylistic convention:

Instantiating AWS clients outside the handler can enhance code legibility and adhere to good coding practices. It makes the code cleaner and more organized by separating client creation from the handler logic. While this improves code readability, it is not the primary benefit but a good practice for maintaining clean, organized code.

#### B. Taking advantage of connection re-use:

This is the primary benefit of instantiating AWS clients outside the handler. When you create AWS clients outside the handler and reuse them across multiple function invocations, you take advantage of connection re-use. Reusing client instances can significantly improve the performance of your Lambda function because it eliminates the overhead of creating new client connections for each invocation. This not only speeds up the function but also reduces costs and network latency. Take advantage of execution environment reuse to improve the performance of your function. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves cost by reducing function run time.

#### C. Better error handling:

While reusing client instances can lead to better error handling by reducing the chances of connection-related errors, it's not the primary benefit. The primary advantage is connection re-use, which contributes to better performance and efficiency. However, improved error handling can be an indirect result of reusing clients.

#### D. Creating a new instance per invocation:

Creating a new client instance per invocation is not a recommended practice, as it can introduce significant overhead in terms of both performance and cost. Creating new instances for each invocation consumes more resources, including memory and CPU, and can slow down your function. Additionally, it may not efficiently reuse existing connections to AWS services, leading to slower response times.

In summary, the primary benefit of instantiating AWS clients outside the scope of the handler is option B: Taking advantage of connection re-use. Reusing client instances improves Lambda function performance, reduces costs, and minimizes network latency, making it a best practice for AWS Lambda development.

---

### Question 97

To refactor and manage the state machine in an architecture where many Lambda functions invoke one another, let's provide detailed explanations for each option:

#### A. AWS Data Pipeline:

AWS Data Pipeline is a service used for data transfer and data processing workflows. It is not specifically designed for managing state machines or coordinating Lambda function invocations. Data Pipeline is more suited for tasks like moving data between different services, transforming data, and orchestrating data-driven workflows.

#### B. AWS SNS with AWS SQS:

Using Amazon Simple Notification Service (SNS) in combination with Amazon Simple Queue Service (SQS) is a way to decouple components in a system and allow them to communicate asynchronously. While this can help with event-driven architectures, it is not a direct solution for managing a state machine or orchestrating complex sequences of Lambda functions.

#### C. Amazon Elastic MapReduce:

Amazon Elastic MapReduce (EMR) is a cloud-native big data platform used for processing and analyzing large datasets. It is not designed for managing state machines or orchestrating Lambda function invocations. EMR is more focused on distributed data processing tasks such as running Hadoop or Spark jobs.

#### D. AWS Step Functions:

AWS Step Functions is a service specifically designed for orchestrating and managing state machines, making it the appropriate choice for this scenario. With AWS Step Functions, you can define state machines using a JSON-based language to coordinate the execution of multiple AWS services, including Lambda functions. It simplifies the process of building and maintaining complex workflows, ensuring reliable and efficient execution.

In summary, the AWS Service that can help refactor and manage the state machine in an architecture with many Lambda functions invoking one another is option D: AWS Step Functions. This service is purpose-built for state machine orchestration and can replace legacy custom code to provide reliable and scalable management of the workflow.

---

### Question 98

To meet the requirements of optimizing the use of Amazon EC2 instances by bin packing containers based on memory reservation in a secure manner, let's provide detailed explanations for each option:

#### A. Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances:

This option involves using an IAM instance profile to provide permissions to the EC2 instances. While this can work for assigning permissions to EC2 instances, it doesn't address the specific requirement of managing IAM roles for ECS services and tasks. The IAM roles assigned to instances are typically used for the EC2 instances themselves and may not directly apply to the ECS services.

#### B. Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS service to reference the associated IAM role:

This option is the most appropriate approach for securely managing permissions for ECS services. By creating separate IAM roles for each ECS service and associating them with the services, you can grant fine-grained permissions to each service. This approach ensures secure access control and isolation of permissions for each service.

#### C. Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then create an IAM group and configure the ECS cluster to reference that group:

While this option involves creating IAM roles for the ECS services, introducing an IAM group and referencing it from the ECS cluster is an unnecessary and potentially confusing step. IAM groups are typically used for organizing and managing users, not for granting permissions to ECS services or tasks. It introduces unnecessary complexity.

#### D. Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to reference the associated IAM role:

This option is similar to option B, but it involves associating IAM roles with ECS task definitions. While this is possible and can work, it is generally better to associate IAM roles directly with the ECS services as it provides a more straightforward and clear way to manage permissions for the services.

In summary, the BEST and MOST secure configuration to meet the requirements of optimizing EC2 instances for ECS services is option B: Create four distinct IAM roles, each containing the required permissions for the associated ECS service, and configure each ECS service to reference the associated IAM role. This approach provides fine-grained control over permissions and ensures security and isolation for each ECS service.

---

### Question 99

To implement a system for order fulfillment that efficiently and simply meets the requirement of making requests to multiple vendors and can take up to a week to complete, let's provide detailed explanations for each option:

#### A. Use AWS Step Functions to execute parallel Lambda functions, and join the results:

AWS Step Functions are a service for coordinating and orchestrating multiple AWS services, including Lambda functions. This option involves creating a Step Function that can execute Lambda functions in parallel and then join their results. Using Step Functions can provide a structured and efficient way to manage complex workflows, especially when you need to coordinate parallel tasks like making requests to multiple vendors. This approach is suitable for long-running processes and offers good visibility into the execution flow. It can be a straightforward way to implement the order fulfillment logic with minimal code. With Step functions, you don't need to implement your all communication model.

#### B. Create an AWS SQS for each vendor, poll the queue from a worker instance, and join the results:

This option suggests using Amazon Simple Queue Service (SQS) for each vendor and having a worker instance poll the queues to process messages. While it's possible to use SQS for asynchronous processing, it may introduce unnecessary complexity in this scenario. Joining results from multiple SQS queues can be cumbersome and requires additional logic to coordinate the workflow. It may not be the most efficient way to implement the order fulfillment system when dealing with a week-long process.

#### C. Use AWS Lambda to asynchronously call a Lambda function for each vendor, and join the results:

This approach involves using Lambda functions to call vendor-specific logic asynchronously. While Lambda is a good choice for serverless execution, orchestrating long-running tasks using this method may become complex, especially if there's a need to coordinate and join the results. Manually managing the orchestration and results joining could be less efficient and simple compared to other solutions designed for workflow orchestration.

#### D. Use Amazon CloudWatch Events to orchestrate the Lambda functions:

Amazon CloudWatch Events is primarily a monitoring and event-driven service, not an orchestration service. It is not well-suited for orchestrating long-running workflows, and this option does not provide a clear solution for joining results. This choice may lack the necessary structure and control for managing a week-long order fulfillment process efficiently.

In summary, the MOST efficient and SIMPLEST way to implement an order fulfillment system that meets the given requirements is option A: Use AWS Step Functions to execute parallel Lambda functions and join the results. AWS Step Functions are designed for workflow orchestration, provide a clear structure for parallel execution, and offer an efficient and organized way to manage complex, long-running processes like order fulfillment.

---

### Question 100

To address intermittent HTTP `400: ThrottlingException` errors when calling the Amazon CloudWatch API, let's provide detailed explanations for each option:

#### A. Contact AWS Support for a limit increase:

This option is not the best practice for addressing ThrottlingException errors. Contacting AWS Support for a limit increase should be considered when you have identified that your application consistently hits service limits, and it's not feasible to optimize or reconfigure your usage further. Throttling usually occurs when you exceed service limits, but it's essential to first investigate whether the current usage can be optimized or if there are other issues causing the throttling.

#### B. Use the AWS CLI to get the metrics:

Using the AWS CLI to retrieve metrics can be helpful for monitoring and troubleshooting, but it does not directly address ThrottlingException errors. This option does not provide a solution for the throttling issue.

#### C. Analyze the applications and remove the API call:

Analyzing the application to identify and remove unnecessary or redundant API calls is a good practice for optimizing usage and potentially addressing throttling issues. This approach focuses on reducing the load on the CloudWatch API by removing unnecessary calls, which can help alleviate throttling issues.

#### D. Retry the call with exponential backoff:

This is the best practice to address ThrottlingException errors. When you receive a ThrottlingException, it indicates that you're hitting rate limits with your API requests. Retry the API call with an exponential backoff strategy, which means you should wait for a short period and then gradually increase the time between retries. This approach helps alleviate the load on the API and can increase the chances of a successful request without hitting the throttle limits.

In summary, the best practice to first address intermittent HTTP `400: ThrottlingException` errors when calling the Amazon CloudWatch API is option D: Retry the call with exponential backoff. This approach is effective in handling transient throttling issues by reducing the load on the API and increasing the likelihood of successful requests without the need for limit increases or other drastic measures.

[ Back to top](#question-51)