[![readme](https://img.shields.io/badge/README-blue)](/)
[![part02](https://img.shields.io/badge/0051--0100-blue)](/questions/0001-0050.md)
[![part02](https://img.shields.io/badge/0051--0100-blue)](/questions/0051-0100.md)
[![part03](https://img.shields.io/badge/0101--0150-blue)](/questions/0101-0150.md)
[![part04](https://img.shields.io/badge/0151--0200-blue)](/questions/0151-0200.md)
[![part05](https://img.shields.io/badge/0201--0250-blue)](/questions/0201-0250.md)
[![part06](https://img.shields.io/badge/0251--0300-blue)](/questions/0251-0300.md)
[![part07](https://img.shields.io/badge/0301--0350-blue)](/questions/0301-0350.md)
[![part08](https://img.shields.io/badge/0351--0400-blue)](/questions/0351-0400.md)
[![part09](https://img.shields.io/badge/0401--0450-blue)](/questions/0401-0450.md)
[![part10](https://img.shields.io/badge/0451--0500-blue)](/questions/0451-0500.md)
[![part11](https://img.shields.io/badge/0501--0550-blue)](/questions/0501-0550.md)
[![part12](https://img.shields.io/badge/0551--0600-blue)](/questions/0551-0600.md)
[![part13](https://img.shields.io/badge/0601--0650-blue)](/questions/0601-0650.md)
[![part14](https://img.shields.io/badge/0651--0700-blue)](/questions/0651-0700.md)
[![part15](https://img.shields.io/badge/0701--0750-blue)](/questions/0701-0750.md)
[![part16](https://img.shields.io/badge/0751--0800-blue)](/questions/0751-0800.md)
[![part17](https://img.shields.io/badge/0801--0850-blue)](/questions/0801-0850.md)
[![part18](https://img.shields.io/badge/0851--0900-blue)](/questions/0851-0900.md)
[![part19](https://img.shields.io/badge/0901--0950-blue)](/questions/0901-0950.md)
[![part20](https://img.shields.io/badge/0951--1000-blue)](/questions/0951-1000.md)
[![part21](https://img.shields.io/badge/1001--1050-blue)](/questions/1001-1050.md)
[![part22](https://img.shields.io/badge/1051--1100-blue)](/questions/1051-1100.md)
[![part23](https://img.shields.io/badge/1101--1150-blue)](/questions/1101-1150.md)
[![part24](https://img.shields.io/badge/1151--1203-blue)](/questions/1151-1203.md)



### Question 102

To allow multiple consumers to process data concurrently and cost-effectively for a real-time processing scenario with millions of events received through an API, let's provide detailed explanations for each option and determine the most suitable service:

#### A. Amazon SNS with fanout to an SQS queue for each application:

Amazon SNS (Simple Notification Service) can fan out messages to multiple subscribers. In this option, SNS is used to distribute events to multiple SQS (Simple Queue Service) queues, with one queue per application. Each application can have its own dedicated queue to process events independently. This approach is cost-effective as it allows you to scale and manage consumers for each application independently, ensuring efficient utilization of resources.

#### B. Amazon SNS with fanout to an SQS FIFO (first-in, first-out) queue for each application:

In this option, the use of SQS FIFO queues means that messages are processed in the order they are received. Each application has its own dedicated FIFO queue for processing events. While this option ensures strict message ordering, it may not be the most cost-effective solution for processing millions of events concurrently because of the added complexity of managing FIFO queues.

#### C. Amazon Kinesis Firehose:

Amazon Kinesis Firehose is a service designed for ingesting and delivering real-time data streams to various AWS services, such as Amazon S3, Amazon Redshift, and Elasticsearch. While Kinesis Firehose can handle high-volume data streams efficiently, it may not be the most cost-effective option when multiple consumers need to process data concurrently. It's primarily designed for delivering data streams to specific destinations, not for distributing data to multiple applications or consumers.

#### D. Amazon Kinesis Streams:

Amazon Kinesis Streams is designed for real-time data streaming and allows multiple consumers to process data concurrently from a stream. Kinesis Streams is a cost-effective option for distributing and processing millions of events concurrently, as it's optimized for handling high-throughput data streams and allows you to scale consumers dynamically.

The most cost-effective and suitable option for processing millions of events concurrently is option D: Amazon Kinesis Streams. It's specifically designed for real-time data streaming scenarios and provides the scalability and performance needed for such use cases, allowing multiple consumers to process data concurrently from the stream.

---

### Question 103

To ingest data at very high throughput from multiple sources and store it in an Amazon S3 bucket, let's provide detailed explanations for each option and determine the service that would BEST accomplish this task:

#### A. Amazon Kinesis Firehose:

Amazon Kinesis Firehose is designed for ingesting and delivering real-time data streams to various AWS services, including Amazon S3. It can capture, transform, and load data into S3. Kinesis Firehose is a suitable choice for scenarios where you need to ingest data at high throughput, perform data transformation, and efficiently load it into an S3 bucket. It is optimized for data streaming use cases.

#### B. Amazon S3 Acceleration Transfer:

Amazon S3 Acceleration Transfer is designed to speed up data transfers to and from Amazon S3 using Amazon CloudFront's globally distributed edge locations. It improves data transfer speeds but does not directly address the ingestion of data from multiple sources. While it can accelerate data transfer, it is not specifically designed for ingesting data at high throughput from many sources.

#### C. Amazon SQS (Simple Queue Service):

Amazon SQS is a managed message queuing service. It is not designed for directly ingesting data into an S3 bucket. SQS is typically used for decoupling components of distributed systems and message-based communication between applications.

#### D. Amazon SNS (Simple Notification Service):

Amazon SNS is a pub/sub messaging service that allows you to publish and subscribe to topics. It is used for notification and messaging purposes, not for directly ingesting data into S3. SNS can be used for notifying subscribers about events, but it is not the most suitable service for ingesting data at high throughput into an S3 bucket.

The service that would BEST accomplish the task of ingesting data at very high throughput from multiple sources and storing it in an Amazon S3 bucket is option A: Amazon Kinesis Firehose. Kinesis Firehose is specifically designed for real-time data streaming and can efficiently capture, transform, and load data into an S3 bucket, making it the most appropriate choice for this use case.

---

### Question 104

To minimize Lambda compute time consumed when using external libraries that are not included in the standard Lambda libraries, let's provide detailed explanations for each option and determine the most suitable action:

#### A. Install the dependencies and external libraries at the beginning of the Lambda function:

This option suggests installing dependencies and external libraries within the Lambda function itself. However, this approach is not efficient because it would result in the installation process running every time the Lambda function is invoked. This increases the function's startup time and consumes additional compute resources.

#### B. Create a Lambda deployment package that includes the external libraries:

This is a recommended approach. You can create a deployment package that includes both your Lambda function code and the external libraries it depends on. By packaging the external libraries with the Lambda function, you eliminate the need for the installation process during each invocation. This minimizes compute time consumed and results in faster execution.

#### C. Copy the external libraries to Amazon S3 and reference the external libraries to the S3 location:

Storing external libraries in Amazon S3 and referencing them in your Lambda function can help reduce the package size but may not minimize compute time. The Lambda function still needs to download these libraries from S3, which can add additional overhead and time.

#### D. Install the external libraries in Lambda to be available to all Lambda functions:

Installing external libraries in the Lambda environment itself makes them available for all Lambda functions within that environment. However, this does not directly minimize compute time for an individual Lambda function. It may benefit other functions that use the same libraries, but it doesn't specifically optimize the function's execution time. Lambda execution environments are ephemeral and don't allow you to install libraries that persist across all Lambda functions. Each Lambda function runs in its own isolated environment. Even if you could install libraries for all functions, it wouldn't be a way to minimize compute time, as the libraries would still need to be loaded during each function invocation.

The most suitable action to minimize Lambda compute time consumed when using external libraries is option B: Create a Lambda deployment package that includes the external libraries. This approach ensures that the external libraries are pre-packaged with the Lambda function, eliminating the need for installation during each function invocation and resulting in faster execution times.

---

### Question 105

To optimize a full Amazon DynamoDB table scan during non-peak hours while minimizing execution time without affecting normal workloads, let's provide detailed explanations for each option:

#### A. Use parallel scans while limiting the rate:

Using parallel scans can improve the efficiency of table scans by dividing the table into smaller segments that can be scanned concurrently. This can significantly reduce the execution time of the scan. However, limiting the rate is also a crucial consideration to avoid consuming too many read capacity units during the scan. By controlling the rate of parallel scans, you can ensure that the scan doesn't impact normal workloads by exceeding available provisioned capacity.

#### B. Use sequential scans:

Sequential scans involve reading items from the table one after another in a linear fashion. While this approach doesn't require additional provisioned capacity, it can result in slower execution times for large tables. Using sequential scans is not the most efficient way to minimize the execution time, especially if the goal is to complete the scan quickly during non-peak hours.

#### C. Increase read capacity units during the scan operation:

Increasing the provisioned read capacity units during the scan operation is a viable option to ensure that the scan completes quickly. DynamoDB allows you to change the provisioned capacity for a table or index. This can help optimize the scan by providing sufficient read capacity to support the increased workload during the scan. However, this option involves increasing costs temporarily during the scan operation.

#### D. Change consistency to eventually consistent during the scan operation:

DynamoDB offers two types of read consistency: strongly consistent and eventually consistent. Strongly consistent reads consume twice as many read capacity units as eventually consistent reads. If the use case allows for eventually consistent reads, switching to eventually consistent reads during the scan operation can help reduce the read capacity units consumed. This can be a cost-effective way to optimize the scan, but it may not provide the same level of consistency.

The most suitable approach to optimize the scan during non-peak hours while minimizing execution time and not affecting normal workloads is option A: Use parallel scans while limiting the rate. This approach combines the benefits of parallel scanning for efficiency while controlling the rate to prevent overconsumption of read capacity units. It allows for a balance between scan performance and resource utilization.

---

### Question 106

To optimize performance for a large e-commerce site delivering static objects from Amazon S3 while serving more than 300 GET requests per second, you can consider the following options:

#### A. Integrate Amazon CloudFront with Amazon S3:

Amazon CloudFront is a Content Delivery Network (CDN) service that caches content at edge locations worldwide. By integrating Amazon CloudFront with your Amazon S3 bucket, you can distribute content closer to end users, reducing latency and improving overall performance. CloudFront caches frequently accessed content, reducing the load on the S3 bucket and providing faster response times to users. This helps efficiently handle the high request rate and reduces the load on the S3 bucket.

#### B. Enable Amazon S3 cross-region replication:

Cross-region replication is used for disaster recovery, compliance, and replicating data to different regions. Enabling cross-region replication can help improve data availability and durability but is not directly related to optimizing performance for serving GET requests. This option is not the most relevant choice for optimizing performance in this scenario.

#### C. Delete expired Amazon S3 server log files:

Deleting expired server log files is a maintenance task and does not directly impact the performance of serving GET requests.

#### D. Configure Amazon S3 lifecycle rules:

Amazon S3 lifecycle rules help automate the management of objects in your S3 buckets, primarily for data retention and cost optimization rather than performance optimization.

#### E. Randomize Amazon S3 key name prefixes:

Randomizing Amazon S3 key name prefixes can help distribute access patterns across different parts of the S3 bucket, potentially reducing contention, but it's not a commonly recommended approach for optimizing performance in this context. Previously Amazon S3 performance guidelines recommended randomizing prefix naming (option E) with hashed characters to optimize performance for frequent data retrievals. From documentation: You no longer have to randomize prefix naming for performance, and can use sequential date-based naming for your prefixes. Refer to the Performance Guidelines for Amazon S3 and Performance Design Patterns for Amazon S3 for the most current information about performance optimization for Amazon S3. This in the past was a perf improvement.

Therefore, the most effective options for optimizing performance when serving static objects from Amazon S3 for a large e-commerce site with high traffic are A (Integrate Amazon CloudFront with Amazon S3) and a more relevant option based on performance considerations (which is not among the given options). This question had a (Select TWO.) tag.

---

### Question 107

The most likely cause of the application latency in this scenario is:
B. The AWS KMS API calls limit is less than needed to achieve the desired performance.

Here's an explanation for each option:

#### A. Amazon S3 throttles the rate at which uploaded objects can be encrypted using Customer Master Keys:

Amazon S3 may throttle requests in some situations, but this is not typically related to encryption performance with AWS KMS. It's more likely that the rate of API calls to AWS KMS is the issue.

#### B. The AWS KMS API calls limit is less than needed to achieve the desired performance:

AWS KMS enforces rate limits on API calls to ensure security and prevent abuse. If your application is uploading tens of thousands of objects per second and each of these objects requires an API call to KMS for server-side encryption with a unique key, you could hit rate limits, leading to application latency.

#### C. The client encryption of the objects is using a poor algorithm:

While using a poor encryption algorithm could impact performance, the question specifically mentions that server-side encryption with AWS KMS is being used. In this case, the encryption is performed on AWS servers and not by the client application.

#### D. KMS requires that an alias be used to create an independent display name that can be mapped to a CM:

This statement is not directly related to the application's latency. AWS KMS aliases are used for human-friendly names for customer master keys (CMKs) and don't typically impact application performance.

Therefore, in this case, it is most likely that the AWS KMS API calls limit is causing the application latency, as the high object upload rate may exceed the rate limits enforced by KMS.

---

### Question 108

The deployment policy that would best satisfy the customer's requirements for deploying source code on an AWS Elastic Beanstalk environment with minimal outage and retaining the application access log, let's provide detailed explanations for each option:

#### A. Rolling:

In a Rolling deployment, Elastic Beanstalk deploys the new version in batches and stops instances one batch at a time. This approach minimizes downtime because it gradually replaces instances. Existing instances are used during the deployment, so it can help retain the application access log on those instances.

#### B. All at once:

In an All at Once deployment, Elastic Beanstalk replaces all instances at the same time, resulting in significant downtime. It doesn't align with the requirement for minimal outage and using existing instances.

#### C. Rolling with an additional batch:

Rolling with an additional batch is a variant of the Rolling deployment. It also replaces instances in batches but creates an additional batch of instances with the new version before stopping the old ones. This can minimize downtime, but it may not specifically retain the application access log on existing instances.

#### D. Immutable:

In an Immutable deployment, Elastic Beanstalk replaces all instances with new ones. This can result in minimal downtime, but it doesn't use existing instances and doesn't guarantee the retention of the application access log on those instances.

So, while option A (Rolling) is the most suitable deployment policy for minimizing downtime and using existing instances during the deployment, which indirectly helps in retaining the application access log.

---

### Question 109

The feature that the Developer should implement to ensure that each sender's messages are processed in the order they are received in an Amazon SQS queue is:

#### A. Configure each sender with a unique MessageGroupId.

Here's an explanation for each option:

#### A. Configure each sender with a unique MessageGroupId:

The MessageGroupId is a SQS attribute that groups related messages together. By configuring each sender with a unique MessageGroupId, you ensure that messages sent by each sender are processed in the order they are received. This is the correct approach to maintain order within the context of each sender.

#### B. Enable MessageDeduplicationIds on the SQS queue:

MessageDeduplicationIds are used to prevent duplicate messages from being sent to the queue. Enabling this feature is helpful for avoiding message duplication but does not guarantee message order within each sender's stream.

#### C. Configure each message with unique MessageGroupIds:

While using unique MessageGroupIds for each message is a valid option, it doesn't ensure that messages from the same sender are processed in the order they are received. It groups messages within the queue but not within the context of each sender.

#### D. Enable ContentBasedDeduplication on the SQS queue:

ContentBasedDeduplication is another feature to prevent duplicate messages, but it doesn't address the requirement of processing messages in the order they are received.

So, option A (Configure each sender with a unique MessageGroupId) is the correct choice to maintain message order for each sender's messages in the SQS queue.

---

### Question 110

Let's provide detailed explanations for each option:

#### A. Apply tags to the Lambda functions:

Explanation: AWS allows you to apply tags to Lambda functions and other resources. Tags are key-value pairs that you can use to add metadata to your resources. While using tags can help with resource organization and identification, they do not directly provide a way for a Lambda function to access different sets of resources in different environments. Tags are primarily for management and identification purposes and do not serve as a direct mechanism for environment-specific resource configuration.

#### B. Hardcore resources in the source code:

Explanation: Hardcoding resources, such as database connection strings or resource endpoints, directly in the source code of your Lambda function is generally not a good practice. It makes your code less portable and maintainable. Hardcoding makes it challenging to switch between different environments, as you would need to modify and redeploy the code for each environment, which is error-prone and can lead to security risks. Therefore, this is not a recommended approach.

#### C. Use environment variables for the Lambda functions:

Explanation: This is the recommended approach for configuring Lambda functions to work in different environments. AWS Lambda allows you to set environment variables for your functions. Environment variables can be used to store configuration values like database connection strings, API endpoints, or any other environment-specific settings. By setting different environment variables for each environment (e.g., development, test, production), you can configure your Lambda function to use the appropriate resources based on the environment it's running in. This approach ensures flexibility and security and simplifies the process of switching between environments.

#### D. Use a separate function for development and production:

Explanation: Creating separate Lambda functions for development and production environments is an option, but it may lead to redundancy and increased management complexity. This approach can work for simple cases, but it's not scalable or efficient for larger systems with multiple environments. It requires maintaining multiple sets of code, which can lead to inconsistencies and errors between development and production. Using environment variables and a single Lambda function that adapts its behavior based on the environment is a more flexible and manageable approach.

In summary, option C (Use environment variables for the Lambda functions) is the recommended approach for configuring Lambda functions to work in multiple environments, as it provides flexibility and security while simplifying the configuration process.

---

### Question 111

#### A. Use the Amazon Cognito user pools to get short-lived credentials for the second account:

Explanation: Amazon Cognito is primarily used for user authentication and identity management for applications. While it can provide short-lived credentials, it's typically used to grant access to AWS resources for authenticated users, not for temporarily accessing resources in a second account. This approach may not be the most secure because it's not designed for cross-account access and may not be suitable for this specific use case.

#### B. Create a dedicated IAM access key for the second account, and send it by mail:

Explanation: Creating a dedicated IAM access key and sending it by mail is not a recommended approach for temporary access to resources in another AWS account. Sending credentials by mail poses security risks, as they can be intercepted or misused. Moreover, this method lacks the ability to enforce time-limited access, making it less secure, and it does not follow best practices for managing temporary access.

#### C. Create a cross-account access role, and use sts:AssumeRole API to get short-lived credentials:

Explanation: This is the most secure and recommended approach for granting temporary access to resources in a second AWS account. You can create a cross-account IAM role in the second account and define the permissions that are needed for the developer. Then, the developer can use the `sts:AssumeRole` API to request temporary credentials from the role in the second account. These temporary credentials are short-lived and provide access based on the defined permissions, and they can be used securely without the need to share permanent IAM access keys. This method also ensures that the access is time-limited and follows AWS best practices for secure access management.

#### D. Establish trust, and add an SSH key for the second account to the IAM user:

Explanation: This option is related to SSH key-based authentication, typically used for accessing EC2 instances and not for granting access to resources in a second AWS account. Using SSH keys and IAM users in this manner is not suitable for securely accessing AWS resources in another account and is not a recommended approach for cross-account access.

In summary, option C (Create a cross-account access role, and use sts:AssumeRole API to get short-lived credentials) is the most secure and recommended way to grant temporary access to resources in a second AWS account. It follows best practices for secure access management, enforces time-limited access, and minimizes the exposure of credentials.

---

### Question 112

To monitor an application deployed on EC2 instances with AWS X-Ray, let's break down each option and explain them in detail:

#### A. Deploy the X-Ray SDK with the application and use X-Ray annotation:

While deploying the X-Ray SDK with the application is a step in the right direction, using X-Ray annotations alone is not the primary method for instrumenting an application on EC2 instances with AWS X-Ray. X-Ray annotations help add metadata to trace segments, but they don't perform the core instrumentation of the application code to capture traces and segments.

#### B. Install the X-Ray daemon and instrument the application code:

This is the correct approach. The X-Ray daemon is a separate process that runs alongside your application on EC2 instances. It collects trace data and sends it to AWS X-Ray. By instrumenting the application code, you ensure that trace data is captured and sent to the X-Ray daemon for further analysis. This is the fundamental method for monitoring applications with AWS X-Ray.

#### C. Install the X-Ray daemon and configure it to forward data to Amazon CloudWatch Events:

While installing the X-Ray daemon is part of the monitoring process, configuring it to forward data to Amazon CloudWatch Events is not the primary function of the X-Ray daemon. The X-Ray daemon is primarily responsible for forwarding data to AWS X-Ray, not CloudWatch Events. You might use CloudWatch Events for additional monitoring and alerting purposes, but it's not the main mechanism for capturing and analyzing trace data.

#### D. Deploy the X-Ray SDK with the application and instrument the application code:

This option suggests deploying the X-Ray SDK with your application, which is a valid approach to instrumenting your code. However, it doesn't mention the use of the X-Ray daemon, which is an essential component for collecting and forwarding trace data to AWS X-Ray. Using the SDK alone might not be sufficient for comprehensive monitoring.

In summary, option B is the correct choice for monitoring an application deployed on EC2 instances with AWS X-Ray, as it correctly emphasizes the installation of the X-Ray daemon and instrumenting the application code to capture and send trace data.

---

### Question 113

In the scenario described, a static website is hosted in an Amazon S3 bucket, and HTML pages on the site use JavaScript to download images from another Amazon S3 bucket, but the images are not displayed when users browse the site. Let's break down the possible causes for this issue based on each option:

#### A. The referenced Amazon S3 bucket is in another region:

This could be a potential cause of the issue. Amazon S3 buckets are region-specific, so if the HTML pages are trying to access images from an S3 bucket located in a different region, there may be latency or connectivity issues. Ensure that both S3 buckets are in the same region or configure cross-region access, which can introduce additional latency.

#### B. The images must be stored in the same Amazon S3 bucket:

This option is not entirely accurate. While it's possible to host a static website and serve its content from a single Amazon S3 bucket, it's not a strict requirement that all assets (including images) must be stored in the same bucket. You can reference objects in other S3 buckets, but you need to ensure that they are accessible and properly configured.

#### C. Port 80 must be opened on the security group in which the Amazon S3 bucket is located:

Amazon S3 doesn't rely on port 80 being open in a security group for users to access content stored in an S3 bucket. S3 is an object storage service, and its access is controlled through permissions and bucket policies rather than traditional port-based firewall rules. This option is not the correct cause of the issue.

#### D. Cross-Origin Resource Sharing must be enabled on the Amazon S3 bucket:

This is a likely cause of the issue. When you load resources (such as images) from a different domain or S3 bucket using JavaScript, you may encounter cross-origin issues. Enabling Cross-Origin Resource Sharing (CORS) on the Amazon S3 bucket hosting the images allows you to specify which domains or origins are permitted to access those resources. Without CORS configured correctly, the browser might block these requests, which could result in the images not being displayed.

In summary, the most probable cause of the issue is option D. Ensuring that Cross-Origin Resource Sharing (CORS) is correctly configured on the Amazon S3 bucket hosting the images should allow the HTML pages to load and display the images without issues. Additionally, it's essential to check the region, but it's not a strict requirement for both buckets to be in the same region.

---

### Question 115

To create a generic IAM policy that can be used for all team members to grant access to their user-specific folders in an Amazon S3 bucket, you should use IAM policy variables. This allows you to create a policy template that can be reused for different team members without specifying each user individually. Let's explore each option:

#### A. Use IAM policy condition:

IAM policy conditions allow you to define rules and constraints for when a policy is applied. While conditions are useful for fine-grained control, they don't directly make the policy generic for multiple users. Conditions can be used in conjunction with variables to create more flexible policies.

#### B. Use IAM policy principle:

The principle in an IAM policy specifies the entity (e.g., a user or role) to which the policy applies. It doesn't inherently make the policy generic. In this case, you want to create a policy that applies to multiple users, so specifying a single principle is not the right approach.

#### C. Use IAM policy variables:

Using IAM policy variables is the correct approach. You can create a policy template that includes variables, such as `${aws:username}`, to dynamically reference the current user's name. This way, the policy can be applied to multiple users, and each user's username is automatically substituted when the policy is evaluated, granting access to their specific folders.

#### D. Use IAM policy resource:

The resource element in an IAM policy specifies the AWS resources to which the policy applies. It doesn't inherently make the policy generic for multiple users. It's more about specifying the target resources rather than the users.

In summary, option C (Use IAM policy variables) is the approach you should use to create a generic IAM policy that can be applied to all team members' user-specific folders in the Amazon S3 bucket. By using variables like `${aws:username}`, you can dynamically customize the policy for each user without the need to create distinct policies for each team member.

---

### Question 116

To meet the requirement of encrypting data at rest with your own master key in AWS-managed services, you would typically use Server-Side Encryption (SSE) with AWS Key Management Service (KMS). Let's explore each option:

#### A. SSE with Amazon S3 (Server-Side Encryption with Amazon S3):

SSE with Amazon S3 allows you to encrypt data stored in Amazon S3 buckets using keys managed by Amazon S3. This option doesn't allow you to use your own master key. It relies on AWS-managed keys, so it doesn't meet the requirement of using your own master key. SSE-S3: AWS manages both data key and master key.

#### B. SSE with AWS KMS (Server-Side Encryption with AWS Key Management Service):

This is the correct option. SSE with AWS KMS allows you to encrypt data at rest in various AWS services, including Amazon S3, using a customer master key (CMK) managed by AWS Key Management Service (KMS). With SSE using KMS, you can use either the AWS-managed key or a customer-managed key (your own master key) to encrypt your data. This aligns with the requirement of using your own master key. SSE-KMS: AWS manages data key and you manage master key.

#### C. Client-side encryption:

Client-side encryption is a technique where data is encrypted on the client-side (e.g., in your application) before being stored in an AWS service. It allows you to use your own keys, but it doesn't leverage AWS-managed services for encryption directly. SSE-C: You manage both data key and master key.

#### D. AWS IAM roles and policies (AWS Identity and Access Management):

AWS IAM roles and policies are used for access control and permissions management but do not directly provide data encryption at rest. They can be used to manage who has access to data and services, but they are not a mechanism for encrypting data at rest.

In summary, option B (SSE with AWS KMS) is the most suitable choice to meet the requirement of encrypting data at rest with your own master key while leveraging an AWS-managed service.

---

### Question 117

To meet the requirements of triggering unit tests in a pipeline for commits to a code repository and being notified of failure events, let's provide detailed explanations for each option:

#### A. Store the source code in AWS CodeCommit. Create a CodePipeline to automate unit testing. Use Amazon SNS to trigger notifications of failure events:

This option is valid. AWS CodeCommit is a source code repository service, and AWS CodePipeline is a continuous integration and delivery service that can automate unit testing. Using Amazon SNS (Simple Notification Service) to trigger notifications of failure events is also a good approach. SNS can send notifications when an event, such as a pipeline failure, occurs. CodePipeline talks directly to SNS.

#### B. Store the source code in GitHub. Create a CodePipeline to automate unit testing. Use Amazon SES to trigger notifications of failure events:

This option stores the source code in GitHub, which is valid if the company prefers to use GitHub. However, SES (Simple Email Service) is not typically used to trigger notifications for pipeline failures. While SES can be used for email notifications, it's not the best choice for pipeline-related notifications.

#### C. Store the source code on GitHub. Create a CodePipeline to automate unit testing. Use Amazon CloudWatch to trigger notifications of failure events:

Storing the source code on GitHub is valid, but using Amazon CloudWatch to trigger notifications of failure events isn't a standard approach. CloudWatch is more focused on monitoring and logging AWS resources. While it can be used to monitor and alert on certain events, it's not the primary service for pipeline failure notifications.

#### D. Store the source code in AWS CodeCommit. Create a CodePipeline to automate unit testing. Use Amazon CloudWatch to trigger notifications of failure events:

This option uses AWS CodeCommit, which is suitable for storing the source code. However, using Amazon CloudWatch to trigger failure notifications is not a standard approach for pipeline notifications. CloudWatch primarily focuses on monitoring and logging.

In summary, option A (store the source code in AWS CodeCommit, use CodePipeline for unit testing, and use Amazon SNS for notifications) is the most appropriate choice for implementing continuous integration, triggering unit tests, and notifying of failure events in the pipeline on AWS.

---

### Question 118

To ensure that no other instances can retrieve a message that has already been processed or is currently being processed in an Amazon SQS queue, you need to work with the visibility timeout of the messages. The visibility timeout specifies the duration for which the message will be invisible to other consumers after it's retrieved by a consumer. Here are the explanations for each option:

#### A. Use the ChangeMessageVisibility API to increase the VisibilityTimeout, then use the DeleteMessage API to delete the message.

This is the correct approach. The default visibility timeout for a message is 30 seconds. When you receive a message from the queue, you can use the ChangeMessageVisibility API to increase the visibility timeout of the message up to 40 seconds, ensuring that it remains invisible to other consumers for a longer duration. After processing the message, you can then use the DeleteMessage API to remove the message from the queue.

#### B. Use the DeleteMessage API call to delete the message from the queue, then call DeleteQueue API to remove the queue.

This approach involves deleting the message as soon as it's processed, which is correct to ensure no other instances process the same message. However, calling DeleteQueue API to remove the queue is not necessary and might interfere with other messages or consumers using the same queue. Deleting the queue is an extreme action and is not typically done for handling individual messages.

#### C. Use the ChangeMessageVisibility API to decrease the timeout value, then use the DeleteMessage API to delete the message.

Decreasing the visibility timeout could be used to make the message visible to other consumers sooner, which is not the goal here. Using the DeleteMessage API after processing is the right step, but decreasing the visibility timeout is not needed and can have unintended consequences.

#### D. Use the DeleteMessageVisibility API to cancel the VisibilityTimeout, then use the DeleteMessage API to delete the message.

There is no DeleteMessageVisibility API in AWS SQS. Therefore, this option is not valid.

In summary, the correct and BEST way to ensure that no other instances can retrieve a message that has already been processed or is currently being processed is to use option A. Increase the visibility timeout using the ChangeMessageVisibility API when receiving the message and then delete the message when processing is complete.

---

### Question 119

To improve security for an application managing financial transactions and implement multi-factor authentication (MFA) as part of the login protocol, you should use a solution that provides MFA support and enhances security. Let's explore each option:

#### A. Amazon DynamoDB to store MFA session data, and Amazon SNS to send MFA codes:

Using Amazon DynamoDB to store MFA session data is not a standard approach for implementing MFA in an application. DynamoDB is typically used for storing structured data, not for managing MFA sessions or codes. Amazon Simple Notification Service (SNS) can be used for sending notifications but is not the primary service for delivering MFA codes. This option is not a standard way to implement MFA.

#### B. Amazon Cognito with MFA:

Amazon Cognito is a service specifically designed for identity and access management, including user authentication and authorization. It provides built-in support for MFA. Using Amazon Cognito with MFA is a recommended and standard approach to implementing MFA for an application. This option is the most appropriate choice.

#### C. AWS Directory Service:

AWS Directory Service is primarily used for integrating on-premises Active Directory environments with AWS services. It is not the primary service for implementing MFA in a custom application. This option is not suitable for the described requirements.

#### D. AWS IAM with MFA enabled:

AWS Identity and Access Management (IAM) allows you to enable MFA for AWS user accounts, but it is intended for managing access to AWS resources and services, not for implementing MFA for a custom application. While it's a useful security feature for AWS accounts, it doesn't directly meet the requirement of implementing MFA in a custom financial transactions application.

In summary, option B (Amazon Cognito with MFA) is the most suitable and standard choice for implementing MFA in a custom application managing financial transactions. It is designed for user authentication, identity management, and MFA support, making it a secure and convenient solution.

---

### Question 120

To allow API customers to invalidate the API cache in Amazon API Gateway, let's explore each option:

#### A. Ask customers to use AWS credentials to call the InvalidateCache API:

This option is not directly related to allowing API customers to invalidate the cache for their individual requests. It involves AWS credentials and an "InvalidateCache" API, which is not a standard approach for customers to control cache invalidation for their requests.

#### B. Ask customers to invoke an AWS API endpoint that invalidates the cache:

This option suggests invoking an AWS API endpoint to invalidate the cache. While it's a valid approach, it's typically used by administrators or developers and might not be suitable for API customers who want to control their cache behavior for their specific requests.

#### C. Ask customers to pass an HTTP header called Cache-Control:max-age=0:

A client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a request that contains the `Cache-Control: max-age=0` header. The client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so. This replaces the existing cache entry with the new response, which is fetched from the integration endpoint.

#### D. Ask customers to add a query string parameter called "INVALIDATE_CACHE" when making an API call:

While this option may be used to implement custom logic for cache invalidation, it's not the standard method described in the documentation. The "Cache-Control: max-age=0" header is a more direct way to request cache invalidation for individual requests.

In summary, option C (Ask customers to pass an HTTP header called Cache-Control:max-age=0) is the most appropriate action to allow API customers to invalidate the cache for their individual requests in Amazon API Gateway. This method aligns with the documented behavior for cache control in the API Gateway.

---

### Question 121

To verify IAM access to get records from Amazon Kinesis Streams when using IAM roles for EC2 instances, let's evaluate the other options:

#### A. Use the AWS CLI to retrieve the IAM group:

This option is not directly related to verifying IAM access to get records from Amazon Kinesis Streams. Using the AWS CLI to retrieve the IAM group would provide information about the IAM group the user is a part of, which might not be relevant to the specific task of verifying access to Kinesis Streams.

#### B. Query Amazon EC2 metadata for in-line IAM policies:

Querying EC2 metadata for in-line IAM policies is not typically done to verify access to Amazon Kinesis Streams. In-line policies are associated with specific IAM users or roles, not with EC2 instances.

#### C. Request a token from AWS STS, and perform a describe action:

Requesting a token from AWS Security Token Service (STS) is a step used to assume an IAM role. It doesn't directly verify IAM access to Amazon Kinesis Streams. This option is about assuming a role, not specifically about Kinesis access. However, if you just do "aws kinesis describe-stream", that does not mean you have "aws kinesis get-records" permission.

#### D. Perform a get action using the --dry-run argument:

`aws kinesis get-records --dry-run`. The --dry-run argument is typically used in the context of AWS CLI or AWS SDK commands to check whether the user or role has permissions to perform an action without actually executing it. This can be useful for verifying IAM access before performing actual operations on resources like Kinesis Streams. This is a valid method for verifying access.

#### E. Validate the IAM role policy with the IAM policy simulator:

The IAM policy simulator is a powerful tool for testing IAM policies to see if they grant or deny specific permissions. This is a suitable option for verifying IAM access to Kinesis Streams. You can simulate actions related to Kinesis Streams to determine if the IAM role's policy allows access.

In summary, the two most suitable actions to verify IAM access to get records from Amazon Kinesis Streams are:
- D. Perform a get action using the --dry-run argument.
- E. Validate the IAM role policy with the IAM policy simulator.

---

### Question 122

To identify the cause of the failures in an AWS CodeBuild project when a recent change to the source code has resulted in compilation issues, you should consider the following options:

#### A. Modify the buildspec.yml file to include steps to send the output of build commands to Amazon CloudWatch:

This option is about adding logging and monitoring to the build process, which is a good practice. By modifying the buildspec.yml file to include steps that send build output to Amazon CloudWatch Logs, you can gather more information about the build process, including any error messages or issues. This is a proactive approach to capturing information during the build.

#### B. Use a custom Docker image that includes the AWS X-Ray agent in the AWS CodeBuild project configuration:

Using the AWS X-Ray agent in a custom Docker image is useful for monitoring and tracing application performance. However, it is more related to performance monitoring and profiling rather than identifying the cause of build failures. This option might not directly help diagnose build issues.

#### C. Check the build logs of the failed phase in the last build attempt in the AWS CodeBuild project build history:

This option is a recommended step to diagnose the cause of build failures. The build logs contain information about each step of the build process, including error messages and issues encountered during compilation. Reviewing the logs is a common and effective approach to identifying the cause of compilation failures.

#### D. Manually re-run the build process on a local machine so that the output can be visualized:

Manually re-running the build process on a local machine can be helpful for debugging purposes, but it's not the most efficient or direct method to identify the cause of build failures. It may not replicate the same environment and dependencies as the CodeBuild environment, and it could be time-consuming.

In summary, the most direct and effective option to identify the cause of build failures is:
- C. Check the build logs of the failed phase in the last build attempt in the AWS CodeBuild project build history.

Option A is also a valuable practice for capturing additional build process information and should be considered as well. However, option C is more directly focused on diagnosing the cause of build failures.

---

### Question 123

In AWS CodeDeploy, hooks are scripts or commands that you can specify to run at various points during the deployment process. These hooks allow you to customize and control the deployment steps to meet your application's specific requirements. In an in-place deployment, the application is updated on the same set of Amazon EC2 instances where the previous version of the application is running. Let's go through each of the options and discuss the run order of hooks for in-place deployments:

Hooks:
- ApplicationStop: This hook stops the previous version of your application as the first step.
- BeforeInstall: After the previous version is stopped, this hook is executed for environment preparation.
- ApplicationStart: After the old version is stopped, if the validation is successful, the new version is started.
- AfterInstall: This hook runs after the new version is installed, allowing you to perform post-installation tasks.
- ValidateService: This is a custom validation step where you can check if the new version is working correctly before continuing.

Deployments:

Standard Deployment:
- BeforeInstall -> ApplicationStop -> ApplicationStart -> AfterInstall.

Rolling (in-place) Deployment:
- ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart.

Blue/Green Deployment with Validation:
- BeforeInstall -> ApplicationStop -> ValidateService -> ApplicationStart.

Blue/Green Deployment with Validation (Different Order):
- ApplicationStop -> BeforeInstall -> ValidateService -> ApplicationStart.

In conclusion, option B (ApplicationStop -> BeforeInstall -> AfterInstall -> ApplicationStart) is the correct run order for hooks in in-place deployments using AWS CodeDeploy. This order ensures a smooth and efficient deployment process with minimal downtime. The choice of hook order depends on your specific deployment strategy and requirements. For example, if you want to minimize downtime, you may choose a rolling deployment. If you want to perform additional validation before making the new version live, you can opt for a blue/green deployment with validation.

---

### Question 124

In AWS Elastic Beanstalk, you can use configuration files to customize and configure your application's environment. The location where you place these configuration files in your application source bundle determines when and how they are applied. For the specific question about the placement of an Elastic Beanstalk configuration file named `healthcheckur1.config`, let's discuss each option:

#### A. In the root of the application:

Placing the `healthcheckur1.config` file in the root of the application source bundle means that it should be at the top level of the bundle, alongside your application code and any other files. In this case, the configuration file will be at the top level of the bundle and will apply to the entire application.

#### B. In the bin folder:

Placing the `healthcheckur1.config` file in the `bin` folder is not a typical location for Elastic Beanstalk configuration files. Configuration files are generally placed outside the application code folders. The `bin` folder is often used for executable binary files and may not be scanned for Elastic Beanstalk configuration.

#### C. In `healthcheckur1.config.ebextension` under the root:

This option suggests creating a directory named `healthcheckur1.config.ebextension` under the root and placing the configuration file inside it. This is incorrect. Elastic Beanstalk expects configuration files to be placed directly in the `.ebextensions` folder, not in a subfolder with the same name.

#### D. In the `.ebextensions` folder:

Placing the `healthcheckur1.config` file in the `.ebextensions` folder is the recommended and standard location for Elastic Beanstalk configuration files. This folder is specifically designed to hold configuration files, and Elastic Beanstalk automatically processes and applies them during environment deployment.

So, the correct answer is **D. In the .ebextensions folder**. This is where you should place the `healthcheckur1.config` file in the application source bundle to ensure that Elastic Beanstalk can properly detect and use it for environment configuration.

---

### Question 125

This question is about integrating a fraud detection solution into an order processing pipeline for a retail company. Given the scalability requirements and the need for a timely response, let's evaluate each option:

#### A. Add all new orders to an Amazon SQS queue. Configure a fleet of 10 EC2 instances spanning multiple AZs with the fraud detection solution installed on them to pull orders from this queue. Update the order with a pass or fail status.

In this option, orders are placed in an SQS queue, and a fixed number of EC2 instances are used for processing. This approach is not scalable as it does not dynamically adjust to the load. It's also not cost-effective because you are maintaining a fixed fleet of instances regardless of the workload. The fraud detection process might not scale well during peak periods.

#### B. Add all new orders to an Amazon SQS queue. Configure an Auto Scaling group that uses the queue depth metric as its unit of scale to launch a dynamically-sized fleet of EC2 instances spanning multiple AZs with the fraud detection solution installed on them to pull orders from this queue. Update the order with a pass or fail status.

This option is more scalable and cost-effective. By using Auto Scaling and scaling based on the queue depth, you can dynamically adjust the number of EC2 instances to handle the incoming load. When the order rate increases during peak times, more instances will automatically be launched to handle the load. This is a better choice for scalability.

#### C. Add all new orders to an Amazon Kinesis Stream. Subscribe a Lambda function to automatically read batches of records from the Kinesis Stream. The Lambda function includes the fraud detection software and will update the order with a pass or fail status.

This option leverages AWS Lambda and Kinesis. While Lambda scales automatically and doesn't require you to manage servers, it may not be the most efficient choice for long-running processes (fraud detection takes 10-30 minutes). Using Lambda for such long-running tasks can lead to higher costs and complexity.

#### D. Write all new orders to Amazon DynamoDB. Configure DynamoDB Streams to include all new orders. Subscribe a Lambda function to automatically read batches of records from the Kinesis Stream. The Lambda function includes the fraud detection software and will update the order with a pass or fail status.

This option combines DynamoDB Streams and Lambda to handle order processing. It has some similarities to option C, where Lambda is used for processing. However, using DynamoDB for this specific use case might not be the most appropriate choice, and long-running Lambda functions can still lead to high costs.

In summary, the most scalable and cost-effective approach for integrating the fraud detection solution into the order processing pipeline is **Option B**. It uses Amazon SQS and an Auto Scaling group that scales based on the queue depth metric, allowing you to dynamically adjust the number of EC2 instances based on the incoming load, making it suitable for handling the peak order rate.

---

### Question 126

The question is asking for the solution that involves the LEAST administrative effort for securely accessing encrypted secrets in an AWS environment while using AWS CodeDeploy to automate application deployment. Let's evaluate each option:

#### A. Save the secrets in Amazon S3 with AWS KMS server-side encryption, and use a signed URL to access them by using the IAM role from Amazon EC2 instances.

In this option, secrets are stored securely in an S3 bucket with KMS encryption, and access is controlled using IAM roles for EC2 instances. While this is a secure method, creating and managing signed URLs for access requires additional administrative effort. You need to generate these signed URLs for each request, which could be cumbersome.

#### B. Use the instance metadata to store the secrets and to programmatically access the secrets from EC2 instances.

Storing secrets in instance metadata is not recommended for security because instance metadata is accessible by any code running on the instance. It lacks security controls and is not an appropriate way to store sensitive information securely. Additionally, programmatically accessing secrets in this manner would involve additional administrative effort to handle security properly.

#### C. Use the Amazon DynamoDB client-side encryption library to save the secrets in DynamoDB and to programmatically access the secrets from EC2 instances.

While using DynamoDB for secret storage is secure, implementing client-side encryption and programmatically accessing secrets involves additional administrative effort. The client-side encryption library would require additional code and configuration. 

#### D. Use AWS SSM Parameter Store to store the secrets and to programmatically access them by using the IAM role from EC2 instances.

AWS Systems Manager (SSM) Parameter Store is designed for storing and managing configuration parameters, including secrets. Using SSM Parameter Store is a secure and easy-to-manage solution. You can control access using IAM roles and policies, and accessing secrets from EC2 instances is straightforward with built-in integration. This option typically involves the least administrative effort for secret management, as it's a managed service.

In summary, the solution that involves the LEAST administrative effort while providing security for accessing secrets is **Option D**. Using AWS SSM Parameter Store simplifies secret management, access control, and is designed for storing sensitive information securely.

---

### Question 127

When an application encounters the error message "The specified bucket does not exist," it means there is a problem with an Amazon S3 bucket, and you need to identify the root cause. Let's evaluate each option for the best place to start the root cause analysis:

#### A. Check the Elastic Load Balancer logs for DeleteBucket requests.

Elastic Load Balancer (ELB) logs are primarily used for monitoring and analyzing traffic to your application. They typically don't provide detailed information about Amazon S3 operations, and DeleteBucket requests should not be generated by an ELB. This option is not the best choice for diagnosing the S3 bucket issue.

#### B. Check the application logs in Amazon CloudWatch Logs for Amazon S3 DeleteBucket errors.

This is a more appropriate choice. Checking the application logs in Amazon CloudWatch Logs for Amazon S3 DeleteBucket errors can help you identify if your application code is trying to access an S3 bucket that doesn't exist or is experiencing issues. It can provide insights into what the application is trying to do with S3 and where the problem might be.

#### C. Check AWS X-Ray for Amazon S3 DeleteBucket alarms.

AWS X-Ray is primarily used for tracing and profiling requests as they move through your application. It's not designed to track S3 bucket operations like DeleteBucket requests. This option would not be suitable for identifying S3-related issues.

#### D. Check AWS CloudTrail for a DeleteBucket event.

AWS CloudTrail is a service specifically designed for recording AWS API activity. Checking CloudTrail logs for a DeleteBucket event is a good choice to determine if someone or something attempted to delete an S3 bucket. If such an event exists, it could help you understand the root cause of the issue. This option is likely the best choice for investigating the error.

In summary, the BEST place to start the root cause analysis of the "The specified bucket does not exist" error is **Option D**, which involves checking AWS CloudTrail for a DeleteBucket event. This can help you identify if a deletion request or some other activity has affected the S3 bucket in question. Who to blame? - CloudTrail, Monitor - CloudWatch, Audit - CloudTrail. Since we are auditing here so D is the correct answer.

---

### Question 128

To ensure that the AWS CLI on a local development server uses the Developer's IAM permissions when making commands, you need to correctly configure the CLI with the appropriate IAM credentials. Let's evaluate each option:

#### A. Specify the Developer's IAM access key ID and secret access key as parameters for each CLI command.

While this is technically possible, it's not a recommended or secure approach. Specifying access key credentials with every command would be cumbersome and could lead to security risks if the credentials are exposed in command history or scripts.

#### B. Run the `aws configure` CLI command, and provide the Developer's IAM access key ID and secret access key.

This is the recommended and standard way to configure the AWS CLI with your IAM credentials. The `aws configure` command prompts you to provide the access key ID, secret access key, default region, and default output format. This information is then stored in the AWS CLI configuration file, making it easy to use the CLI with the appropriate IAM permissions.

#### C. Specify the Developer's IAM user name and password as parameters for each CLI command.

This is not a secure or practical approach for configuring the AWS CLI. It's not common to provide the IAM user name and password as parameters for CLI commands, and it could lead to security risks.

#### D. Use the Developer's IAM role when making the CLI command.

This is a valid approach, but it typically involves switching roles using the `aws sts assume-role` command, which is used for assuming an IAM role temporarily. It's not a standard way to configure the AWS CLI for day-to-day use. While it can be useful for certain scenarios, it's not the primary method for setting up CLI credentials. This is a "local" server, not an EC2 instance, so IAM role is not working in this scenerio.

In summary, the best and most secure way to ensure that the AWS CLI uses the Developer's IAM permissions when making commands is **Option B**. Running the `aws configure` command and providing the IAM access key ID and secret access key is the standard method for configuring the CLI with the necessary credentials.

---

### Question 129

In this scenario, S3 event notifications trigger a Lambda function to resize images stored in an S3 bucket. The key consideration is how AWS Lambda handles the additional traffic. Let's evaluate each option:

#### A. Lambda will scale out to execute the requests concurrently.

This option is correct. AWS Lambda automatically scales out to handle concurrent requests. When S3 event notifications trigger Lambda, Lambda can process multiple invocations concurrently, depending on the incoming traffic. Each invocation is typically stateless, and Lambda can manage multiple concurrent executions efficiently.

#### B. Lambda will handle the requests sequentially in the order received.

This statement is incorrect. AWS Lambda is designed to handle requests concurrently, not sequentially. It doesn't process requests strictly in the order they are received. It can process multiple requests at the same time, and the order in which they are processed may vary.

#### C. Lambda will process multiple images in a single execution.

While AWS Lambda can process multiple records within a single invocation, this depends on the event source. In this scenario, the event source is S3, and it triggers Lambda for each image uploaded, typically resulting in one Lambda invocation per image. Lambda doesn't inherently group multiple images into a single execution.

#### D. Lambda will add more compute to each execution to reduce processing time.

AWS Lambda does not dynamically add more compute resources to a single execution. It scales out by running multiple executions in parallel to handle concurrent requests. Each Lambda execution has its allocated compute resources based on the configured memory size. If you want to reduce processing time, you might increase the allocated memory, which also increases CPU power, but it doesn't add more compute to a single execution.

So, the correct answer is **A. Lambda will scale out to execute the requests concurrently**. AWS Lambda efficiently manages additional traffic by processing requests concurrently and scaling out as needed.

---

### Question 130

To meet latency requirements and reduce the cost of running the stock trading application while using Amazon DynamoDB, let's evaluate each option:

#### A. Add Global Secondary Indexes for trading data.

Adding Global Secondary Indexes (GSI) can help with querying data more efficiently and potentially reduce latency. GSIs allow you to query data in different ways, and if the queries you need for trading operations are not well-supported by the primary key, GSIs can be beneficial. However, while they can improve query performance, they may not be sufficient to meet sub-millisecond latency on their own, especially if there are high spikes in the number of requests. Over-provisioning read capacity can also lead to cost concerns.

#### B. Store trading data in Amazon S3 and use Transfer Acceleration.

Storing trading data in Amazon S3 is not a direct solution for improving DynamoDB's query performance. While S3 is great for storage, it's not a suitable database replacement, especially for low-latency requirements like a stock trading application. Transfer Acceleration helps with data transfer speed to and from S3 but doesn't address query latency issues in DynamoDB.

#### C. Add retries with exponential back-off for DynamoDB queries.

Adding retries with exponential back-off is a good practice to handle transient issues, such as occasional DynamoDB throttling. It can help in terms of resilience. However, it may not be sufficient to meet the sub-millisecond latency requirement, especially during high spikes in request traffic.

#### D. Use DynamoDB Accelerator to cache trading data.

This is the most relevant option for improving both latency and cost efficiency. Amazon DynamoDB Accelerator (DAX) is an in-memory cache that can significantly reduce read latency for frequently accessed data. By caching trading data in DAX, you can achieve sub-millisecond latency for queries. Additionally, it can help reduce the need for over-provisioning read capacity, which can be a cost-saving measure.

In summary, the best option to meet the latency requirements and reduce the cost of running the stock trading application with DynamoDB is **D. Use DynamoDB Accelerator to cache trading data**. This solution not only improves query performance but also helps manage cost by reducing the need for over-provisioning read capacity.

---

### Question 131

In this scenario, the Developer is experiencing an issue where the Lambda function is being executed, but there is no log data generated in Amazon CloudWatch Logs. Let's evaluate each option for potential causes of this situation:

#### A. The Lambda function does not have any explicit log statements for the log data to send to CloudWatch Logs.

If lambda was executed, it will always have some logs to send. For example execution time and memory usage will be always send after execution is done.

#### B. The Lambda function is missing CloudWatch Logs as a source trigger to send log data.

This option is not a direct cause of the issue. Lambda functions do not require CloudWatch Logs as a source trigger to send log data. Log data is typically generated by the Lambda function itself and sent to CloudWatch Logs. CloudWatch Logs can be used for monitoring and troubleshooting, but it's not a trigger source for Lambda.

#### C. The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs.

This is a likely cause of the issue. To send log data to CloudWatch Logs, the Lambda function's execution role must have the necessary permissions to write log data to CloudWatch Logs. If the execution role lacks these permissions, the log data won't be delivered.

#### D. The Lambda function is missing a target CloudWatch Log group.

This is not a typical cause of the issue. Lambda functions do not explicitly target CloudWatch Log groups. Log data generated by a Lambda function is automatically sent to CloudWatch Logs, and it's stored in log groups based on the function's name and version.

In summary, the most likely cause of the issue is **C. The execution role for the Lambda function is missing permissions to write log data to CloudWatch Logs**. Ensure that the execution role associated with the Lambda function has the necessary permissions to write log data to CloudWatch Logs.

---

### Question 132

When dealing with thousands of PUT requests each second to a single Amazon S3 bucket, it's essential to optimize performance to prevent potential bottlenecks. The choice of folder and file naming conventions can have a significant impact on S3 performance.
Let's evaluate each option:

#### A. Prefix folder names with user ID; for example, s3://BUCKET/2013-FOLDERNAME/FILENAME.zip.

This option uses user IDs as a prefix in folder names. While this can help to organize data, it might not be the most efficient choice for performance optimization. It can lead to hotspots if many users are writing to the same folder, as all the writes will be directed to that specific folder.

#### B. Prefix file names with timestamps; for example, s3://BUCKET/FOLDERNAME/2013-26-05-15-00-00-FILENAME.zip.

This option prefixes file names with timestamps. Timestamps can provide uniqueness and distribution of objects over time. This is often a better choice compared to using user IDs in terms of performance. However, if many objects have the same timestamp, it might still lead to some hotspots.

#### C. Prefix file names with random hex hashes; for example, s3://BUCKET/FOLDERNAME/23a6-FILENAME.zip.

This option prefixes file names with random hex hashes. Using random hex hashes can be an excellent choice for optimizing S3 performance. It ensures that objects are distributed randomly across the S3 infrastructure, reducing hotspots and enabling high write throughput.

#### D. Prefix folder names with random hex hashes; for example, s3://BUCKET/23a6-FOLDERNAME/FILENAME.zip.

This option prefixes folder names with random hex hashes. Similar to option C, this can be a good choice for optimizing S3 performance because it helps distribute objects across different prefixes and avoids hotspots.

In summary, the best option for optimizing performance with thousands of PUT requests each second to a single Amazon S3 bucket is **Option C** or **Option D**. Both of these options use random hex hashes in the naming convention, which helps distribute objects across the S3 infrastructure more evenly, reducing the risk of hotspots and improving performance. In S3, have no such thing is folder. "Folder" just a prefix of an object name for easy organizational, so if you want to random prefix object name, you must prefix "folder". Anyway it is no longer necessary

---

### Question 133

To obtain unique identifiers for users who may use multiple devices to access the application, you need a solution that ensures uniqueness across users and devices. Let's evaluate each option:

#### A. Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers.

This approach relies on creating a user table in DynamoDB and storing user-device relationships as key-value pairs. While this approach can work, it's primarily for managing relationships and may not be suitable as a unique identifier for users themselves.

#### B. Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys.

Using IAM-generated access key IDs as unique identifiers for users is not the typical use case for IAM. IAM (Identity and Access Management) is designed for managing access to AWS services, not as a user identification system.

#### C. Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities.

This is a valid approach. Amazon Cognito offers identity management and authentication services. With developer-authenticated identities, you can obtain unique identifiers for users, regardless of the device they use. This allows you to manage user identities securely, and Cognito provides various authentication methods and user management capabilities.

#### D. Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier.

While IAM provides user management and access control, it's primarily designed for managing access to AWS resources rather than serving as a user identification system. Using IAM resource IDs as unique identifiers might not be practical for user identification across devices.

The best choice for obtaining unique identifiers for users who may use multiple devices is **Option C**. Amazon Cognito, with developer-authenticated identities, provides a purpose-built solution for user identification, authentication, and managing user profiles, and it allows you to obtain unique identifiers for users while supporting various authentication methods.

---

### Question 134

To use the AWS CLI to launch a templatized serverless application, you'll generally use AWS CloudFormation to create and manage the resources. Here are the steps to do it:

#### A. Use AWS CloudFormation get-template then CloudFormation execute-change-set.

This option is not the correct sequence of commands for launching a templatized serverless application. The `get-template` and `execute-change-set` commands are typically used for creating and executing changes to a CloudFormation stack, but they are not the first steps in deploying a new serverless application.

#### B. Use AWS CloudFormation validate-template then CloudFormation create-change-set.

This option is closer to the correct sequence. You should validate the CloudFormation template with `validate-template` to ensure it's properly formatted. However, it's not the complete sequence. The `create-change-set` command is used for creating a change set to preview the changes to your stack. It's not typically the first step in deploying a new application.

#### C. Use the AWS CloudFormation package then CloudFormation deploy.

This is the correct sequence for deploying a serverless application using AWS CloudFormation. You should first package the application using the `package` command, which prepares your code for deployment and uploads it to an S3 bucket. Then, you can use the `deploy` command to deploy the application based on the packaged artifacts. 1. sam package --template-file sam.yaml --s3-bucket mybucket --output-template-file packaged.yaml. 2. sam deploy --template-file ./packaged.yaml --stack-name mystack --capabilities CAPABILITY_IAM

#### D. Use AWS CloudFormation create-stack then CloudFormation update-stack.

This sequence is also related to CloudFormation, but it's used for creating and updating stacks, not necessarily for launching a new serverless application from a template. Using `create-stack` and `update-stack` commands would be used when you already have a CloudFormation stack and need to create or update resources within it.

So, the correct sequence for using the AWS CLI to launch a templatized serverless application is **C. Use the AWS CloudFormation package then CloudFormation deploy**. This sequence is designed for deploying serverless applications using AWS CloudFormation.

---

### Question 135

In this scenario, a deployment package is using the AWS CLI to copy files into S3 buckets in the AWS account. The package is running on EC2 instances with an assumed IAM role and a more restrictive policy that should allow access to only one bucket. However, the Developer can still write into all S3 buckets in that account. Let's evaluate each option for the most likely cause of this situation:

#### A. An IAM inline policy is being used in the IAM role.

An inline policy, if used in the IAM role, could potentially override permissions provided by managed policies. However, it would require someone to intentionally configure and add an inline policy to the IAM role. While it's a possible cause, it may not be the most likely one.

#### B. An IAM-managed policy is being used in the IAM role.

Managed policies are typically attached to IAM roles and can be centrally managed. If a managed policy attached to the IAM role is not restrictive enough or has broader permissions than intended, it could allow the Developer to access more S3 buckets than expected. This is a more likely cause, as someone might have modified the IAM managed policy, either intentionally or unintentionally.

#### C. The AWS CLI is corrupt and needs to be reinstalled.

While a corrupt AWS CLI installation can cause issues, it wouldn't typically result in bypassing IAM role restrictions. The AWS CLI should respect the permissions granted by the IAM role. It's less likely to be the root cause in this situation.

#### D. The AWS credential provider looks for instance profile credentials last.

This is the correct answer. By default, the AWS credential provider looks for credentials in a specific order: environment variables, configuration files, and finally, instance profile credentials. If the AWS CLI is using access keys stored in environment variables and those are overriding the instance profile credentials, it could explain why the Developer still has access to all S3 buckets. The environment variables take precedence over instance profile credentials in the default credential provider order.

So, the **MOST likely cause of this situation is Option D**. The AWS credential provider, by default, looks for instance profile credentials last, which means that the access keys stored in environment variables may be overriding the more restrictive IAM role assigned to the EC2 instances.

---

### Question 136

When the length of all environment variables in an AWS CodeBuild project exceeds the limit for the combined maximum number of characters, you need to find a solution to store and manage these variables more effectively. Let's evaluate each option:

#### A. Add the export LC_ALL="en_US.utf8" command to the pre_build section to ensure POSIX localization.

This option suggests using a localization setting but does not directly address the issue of exceeding the character limit for environment variables. Changing the localization setting might have some impacts on character encoding but is not a recommended solution for managing large environment variable sets.

#### B. Use Amazon Cognito to store key-value pairs for large numbers of environment variables.

Amazon Cognito is primarily used for identity management and authentication, not for storing and managing environment variables. This option does not address the problem of exceeding the character limit for environment variables.

#### C. Update the settings for the building project to use an Amazon S3 bucket for large numbers of environment variables.

Storing environment variables in an Amazon S3 bucket is a viable approach when you have a large number of variables that exceed the character limit. CodeBuild allows you to reference an S3 bucket for environment variable values, and this is a suitable solution for managing a large set of variables.

#### D. Use AWS Systems Manager Parameter Store to store large numbers of environment variables.

AWS Systems Manager Parameter Store is a service designed for storing and managing configuration data, secrets, and parameter values. It's a recommended approach for managing large sets of environment variables, especially when you're dealing with sensitive data or complex configurations. This option is a valid solution.

The recommended solution for managing large numbers of environment variables when exceeding the character limit is **Option D: Use AWS Systems Manager Parameter Store**. It's a dedicated service for managing configuration data, and it can help you efficiently store and retrieve environment variable values for your CodeBuild project.

---

### Question 137

To ensure that a real-time dashboard web application is kept up to date with the state of objects in Amazon S3 buckets in the most optimal and cost-effective manner, let's evaluate each option:

#### A. Use an Amazon CloudWatch event backed by an AWS Lambda function. Issue an Amazon S3 API call to get a list of all Amazon S3 objects and persist the metadata within DynamoDB. Have the web application poll the DynamoDB table to reflect this change.

This option suggests using CloudWatch events and Lambda to periodically update DynamoDB. However, it uses a scheduled approach, requiring regular polling by the web application to stay up to date. This can be resource-intensive and may not provide true real-time updates. Additionally, it may result in unnecessary Lambda executions, which can add to costs.

#### B. Use Amazon S3 Event Notification backed by a Lambda function to persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change.

This option is a more cost-effective and efficient approach. Using Amazon S3 Event Notifications, you can trigger a Lambda function whenever objects are created, modified, or deleted in the S3 buckets. The Lambda function can then update DynamoDB in real-time based on these events. This eliminates the need for web application polling and provides real-time updates. It's a more event-driven and cost-effective approach.

#### C. Run a cron job within an Amazon EC2 instance to list all objects within Amazon S3 and persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change.

This option involves running a cron job within an EC2 instance to periodically update DynamoDB, similar to Option A. It's not as efficient or cost-effective as an event-driven approach, as it involves unnecessary polling, additional EC2 resources, and less real-time responsiveness.

#### D. Create a new Amazon EMR cluster to get all the metadata about Amazon S3 objects; persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change.

Using Amazon EMR to collect S3 metadata is overkill for this scenario. EMR clusters are typically used for data processing at scale, such as large-scale data analytics. It introduces unnecessary complexity and costs for a simple metadata update task.

The most optimal and cost-effective design for keeping the real-time dashboard up to date with the state of Amazon S3 objects is **Option B**. By using Amazon S3 Event Notifications and Lambda, you can have real-time updates with minimal additional costs, efficient resource utilization, and a responsive dashboard. It is the most event-driven and cost-effective solution.

---

### Question 138

To understand why an application may sometimes retrieve the old version of an object from Amazon S3 immediately after overwriting it, let's evaluate each option:

#### A. S3 overwrite PUTS are eventually consistent, so the application may read the old object.

This option correctly identifies the root cause of the issue. Amazon S3 provides eventual consistency for overwrite PUTS, which means that it may take some time for all copies of the object to be updated. During this period, you might read the old version of the object.

#### B. The application needs to add extra metadata to label the latest version when uploading to Amazon S3.

While adding metadata can be helpful for versioning and managing objects, it doesn't directly address the issue of eventual consistency. The application may still read the old version immediately after overwriting.

#### C. All S3 PUTS are eventually consistent, so the application may read the old object.

Not all S3 PUT operations are eventually consistent. New object uploads and overwrites typically provide strong read-after-write consistency, but there may be brief periods of eventual consistency for some read operations in the case of overwrites and deletes.

#### D. The application needs to explicitly specify the latest version when retrieving the object.

While versioning can be used in Amazon S3 to manage different versions of objects, specifying the latest version doesn't necessarily address the issue of eventual consistency when immediately reading the object after overwriting it. This is because the overwrite PUT itself is subject to eventual consistency.

The correct explanation for the behavior is **Option A**. S3 overwrite PUTS are eventually consistent, meaning that there may be a delay before all copies of the object across S3 storage are updated. As a result, the application may sometimes retrieve the old version of the object immediately after overwriting it. - This is an old question. According to documentation (2023-10-30), Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of objects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both writes to new objects as well as PUT requests that overwrite existing objects and DELETE requests. A process replaces an existing object and immediately tries to read it. Amazon S3 will return the new data.

---

### Question 139

To develop an application that stores hundreds of video files with unique keys for each video file, you should consider various security and encryption options. Let's evaluate each option:

#### A. Use the KMS Encrypt API to encrypt the data. Store the encrypted data key and data.

In this option, the KMS Encrypt API is used to encrypt the data. While it encrypts the data, it doesn't address the requirement of having unique keys for each video file. Storing the encrypted data key along with the data is a good practice to ensure you can decrypt the data, but it doesn't provide a unique key for each video file.

#### B. Use a cryptography library to generate an encryption key for the application. Use the encryption key to encrypt the data. Store the encrypted data.

This option suggests using a custom cryptography library to generate encryption keys for the application and using those keys to encrypt the data. Storing the encrypted data is a good practice, but this option doesn't specify a unique key for each video file, which is a requirement in the question.

#### C. Use the KMS GenerateDataKey API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data.

This is the correct option. It recommends using the KMS GenerateDataKey API, which allows you to create a unique data key for each video file. You then use this data key to encrypt the data, and you store the encrypted data key along with the encrypted data. This approach ensures a unique key for each video file and aligns with best practices for data encryption.

#### D. Upload the data to an S3 bucket using server-side encryption with an AWS KMS key.

This option focuses on server-side encryption when uploading data to an S3 bucket using AWS KMS for encryption. While it ensures that data at rest is encrypted, it doesn't address the requirement for unique keys for each video file, which is a critical aspect of the question.

The best choice to meet the requirement of unique encryption keys for each video file is **Option C**. It uses the KMS GenerateDataKey API to create unique data keys for each video file, ensuring proper encryption and unique keys for each object.

---

### Question 140

When dealing with a Lambda function that is failing to process after two retries, it's important to troubleshoot and investigate the failure. Let's evaluate each option:

#### A. Configure AWS CloudTrail logging to investigate the invocation failures.

AWS CloudTrail logs API activity and can provide valuable information for troubleshooting. However, it primarily captures management events and API calls, rather than the detailed processing of an AWS Lambda function invocation. It may not provide specific information about why the Lambda function failed after retries.

#### B. Configure Dead Letter Queues by sending events to Amazon SQS for investigation.

This is the correct answer. AWS Lambda supports the concept of Dead Letter Queues (DLQs) for asynchronous invocation. When a Lambda function fails after retries, you can configure a DLQ to capture the failed events, providing a mechanism to investigate and retry them separately. By sending events to an Amazon SQS queue, you can examine the events, identify the root cause of the failure, and take corrective actions.

#### C. Configure Amazon Simple Workflow Service to process any direct unprocessed events.

Amazon Simple Workflow Service (SWF) is designed for managing coordination and stateful workflows, and it may not be the most appropriate solution for troubleshooting Lambda failures. It's typically used for more complex orchestration needs.

#### D. Configure AWS Config to process any direct unprocessed events.

AWS Config is a service for assessing, auditing, and evaluating the configurations of AWS resources. It doesn't directly relate to troubleshooting Lambda function failures or handling retries.

The **best choice for troubleshooting the failure of a Lambda function after two retries** is **Option B: Configure Dead Letter Queues by sending events to Amazon SQS for investigation**. This option provides a structured and effective way to capture and investigate failed events, allowing you to determine the root cause of the failure and take appropriate corrective actions.

---

### Question 141

To limit the amount of requests end users can send in Amazon API Gateway while offering registered developers the option of buying larger packages with the least amount of overhead management, let's evaluate each option:

#### A. Enable throttling for the API Gateway stage. Set a value for both the rate and burst capacity. If a registered user chooses a larger package, create a stage for them, adjust the values, and share the new URL with them.

This option involves enabling throttling for the API Gateway stage, which is a good practice for limiting request rates. However, creating a separate stage for each registered user with a larger package and adjusting the values manually for each user can be cumbersome and requires more management overhead.

#### B. Set up Amazon CloudWatch API logging in API Gateway. Create a filter based on the user and requestTime fields and create an alarm on this filter. Write an AWS Lambda function to analyze the values and requester information, and respond accordingly. Set up the function as the target for the alarm. If a registered user chooses a larger package, update the Lambda code with the values.

This option involves complex custom solutions and requires writing custom Lambda functions to handle rate limiting based on CloudWatch alarms and user packages. It introduces a high degree of management and maintenance overhead.

#### C. Enable Amazon CloudWatch metrics for the API Gateway stage. Set up CloudWatch alarms based on the Count metric and the ApiName, Method, Resource, and Stage dimensions to alert when request rates pass the threshold. Set the alarm action to Deny. If a registered user chooses a larger package, create a user-specific alarm and adjust the values.

This option uses CloudWatch alarms based on metrics and is a step in the right direction. However, creating user-specific alarms and adjusting values manually for each user still introduces management overhead.

#### D. Set up a default usage plan, specify values for the rate and burst capacity, and associate it with a stage. If a registered user chooses a larger package, create a custom plan with the appropriate values and associate the plan with the user.

This is the most efficient and scalable approach. You can create a default usage plan with rate and burst capacity values, and then create custom plans with appropriate values for registered users. This allows you to manage rate limits for different users by associating them with the relevant usage plan. It minimizes manual adjustments and management overhead.

The option with the **least amount of overhead management** is **Option D**. It allows you to create custom usage plans for registered developers with larger packages and associate those plans with the respective users. This approach is more scalable and efficient than manually adjusting values or writing custom code.

---

### Question 142

When refactoring a monolithic application into individual AWS Lambda functions and needing to invoke the Lambda functions in the same sequence using Amazon API Gateway, the optimal approach is to use AWS Step Functions. Let's evaluate each option:

#### A. Use Amazon SQS to invoke the Lambda functions.

Using Amazon SQS to invoke Lambda functions is typically suitable for decoupling components and handling asynchronous processing. While it's possible to invoke Lambda functions from SQS, it doesn't provide a straightforward way to ensure a specific sequence or orchestration of Lambda functions, making it less suitable for this specific use case.

#### B. Use an AWS Step Functions activity to run the Lambda functions.

This option is a part of the correct solution. AWS Step Functions is designed for orchestrating and sequencing AWS services, including Lambda functions. By defining a state machine in Step Functions, you can ensure that Lambda functions are executed in a specific sequence and orchestration.

#### C. Use Amazon SNS to trigger the Lambda functions.

Amazon SNS (Simple Notification Service) is more suitable for pub/sub and event-driven scenarios. While you can use SNS to trigger Lambda functions, it doesn't inherently provide orchestration capabilities to ensure a specific sequence of execution.

#### D. Use an AWS Step Functions state machine to orchestrate the Lambda functions.

This is the most appropriate solution. AWS Step Functions provides a dedicated service for orchestrating and sequencing AWS services, including Lambda functions. With Step Functions, you can define a state machine that specifies the sequence in which Lambda functions should be executed, ensuring the correct orchestration of the functions.

The correct choice for invoking the Lambda functions in the same sequence using Amazon API Gateway is **Option D: Use an AWS Step Functions state machine to orchestrate the Lambda functions**. It provides the necessary orchestration and sequencing capabilities for ensuring that Lambda functions are executed in the desired order.

---

### Question 143

To provide the required transactional capability for updating both users' records as a single transaction or rolling back the changes, you should consider the following AWS database options:

#### A. Amazon DynamoDB with operations made with the ConsistentRead parameter set to true.

This option refers to enabling strong consistency for DynamoDB reads by setting the `ConsistentRead` parameter to true. While this ensures consistent reads, it doesn't inherently provide the ability to perform multi-table or multi-item transactions.

#### B. Amazon ElastiCache for Memcached with operations made within a transaction block.

Amazon ElastiCache for Memcached is an in-memory caching service and does not inherently provide the capability for performing multi-step transactions for updating users' records as described.

#### C. Amazon Aurora MySQL with operations made within a transaction block.

This is a suitable option. Amazon Aurora, based on MySQL, supports transactions and is a relational database service that can be used to ensure both users' records are updated as a single transaction or rolled back if necessary.

#### D. Amazon DynamoDB with reads and writes made by using Transact* operations.

This is a valid option. Amazon DynamoDB provides TransactWriteItems and TransactGetItems operations that allow you to perform multiple reads and writes within a single transaction. This is suitable for updating both users' records as a single transaction.

#### E. Amazon Redshift with operations made within a transaction block.

Amazon Redshift is a data warehousing service, and while it supports SQL transactions, it may not be the most suitable option for the specific use case described, as it's primarily designed for analytics and data warehousing rather than online transaction processing.

The two AWS database options that can provide the required transactional capability for the new feature are:

#### C. Amazon Aurora MySQL with operations made within a transaction block.
D. Amazon DynamoDB with reads and writes made by using Transact* operations.

These options allow you to ensure that both users' records are updated in a transactional manner, providing consistency and the ability to roll back the transaction if needed.

---

### Question 144

To accomplish the task of generating a new file in an AWS Lambda function and checking it into an AWS CodeCommit repository hosted in the same AWS account, let's evaluate each option:

#### A. When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change.

While this option is theoretically possible, it's not the most efficient or practical approach. Lambda functions are stateless, and performing Git operations within the Lambda function, such as cloning and pushing, can be complex, require additional setup, and may not work well in a serverless context.

#### B. After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository.

This approach involves invoking the CodeCommit API using cURL, which is a potential solution. However, it requires manually managing the interaction with the API and authentication, which can be more cumbersome than using AWS SDKs or services designed for such tasks.

#### C. Use an AWS SDK to instantiate a CodeCommit client. Invoke the PutFile method to add the file to the repository.

This is the most suitable option. Using an AWS SDK (such as the AWS SDK for Python or AWS SDK for Node.js), you can instantiate a CodeCommit client and use the `PutFile` method to add the new file to the repository. This approach is more efficient, secure, and well-suited for this use case.

#### D. Upload the new file to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository.

While this option introduces a pipeline with Amazon S3 and AWS Step Functions, it adds unnecessary complexity for the specific task described. You can directly add the new file to the CodeCommit repository without involving S3 and Step Functions.

The most efficient and straightforward approach for adding the new file to the CodeCommit repository from an AWS Lambda function is **Option C**: Use an AWS SDK to instantiate a CodeCommit client and invoke the `PutFile` method to add the file to the repository. This approach aligns well with best practices and the Lambda function's capabilities.

---

### Question 145

To keep IAM credentials used by an application in Amazon EC2 secure and ensure they are not misused or compromised, let's evaluate each option:

#### A. Environment variables.

Storing IAM credentials in environment variables is not considered a secure practice. Environment variables can be accessed and viewed by users with access to the system, and they can potentially be exposed in logs or other ways. It's not a recommended approach for securing IAM credentials.

#### B. AWS credentials file.

The AWS credentials file, typically located at `~/.aws/credentials`, is a secure way to store and manage AWS IAM credentials. This file is used by AWS CLI, SDKs, and other tools to access AWS services. You can configure named profiles in this file, and each profile can have its own set of IAM credentials. By using the AWS credentials file, you can keep IAM credentials secure and separate from the application code or environment.

#### C. Instance profile credentials.

This is the most secure and recommended approach for IAM credentials in EC2 instances. EC2 instances can be associated with an IAM role, and this role can provide temporary credentials to the instance. These credentials are automatically rotated and managed by AWS, and the application can use them without the need to store or manage IAM credentials manually. It's a best practice to use instance profile credentials when running applications on EC2.

#### D. Command line options.

Passing IAM credentials via command line options is not a secure approach. Command line options can be visible in the command history, logs, and process lists. Storing credentials in this way can expose them to potential misuse or compromise.

The most secure and recommended approach for keeping IAM credentials used by an application in Amazon EC2 secure is **Option C: Instance profile credentials**. By associating the EC2 instance with an IAM role, the application can access temporary, automatically rotated IAM credentials without the need for manual management or storage. This approach is designed to enhance security and reduce the risk of misuse or compromise.

---

### Question 146

To provide access to both registered and guest users for reading objects from an Amazon S3 bucket while considering scalability, security, and AWS best practices, let's go through each option and provide a detailed explanation for their suitability in this scenario:

#### A. Provide a different access key and secret access key in the application code for registered users and guest users to provide read access to the objects.

Explanation: This approach is not recommended. Embedding access keys and secret access keys in the application code is not a secure practice, as it poses a significant security risk. Access keys should be kept confidential and not hardcoded in your application, as they can be easily compromised if the code is exposed or leaked.

#### B. Use S3 bucket policies to restrict read access to specific IAM users.

Explanation: Using S3 bucket policies is a good practice for managing access to objects in an S3 bucket. You can specify the permissions and restrictions for IAM users and roles using bucket policies. However, this option doesn't directly address the differentiation between registered users and guest users, as bucket policies work at the IAM user or role level. You could potentially use conditions within the policy to distinguish user types, but it might not be the most straightforward approach.

#### C. Use Amazon Cognito to provide access using authenticated and unauthenticated roles.

Explanation: Amazon Cognito is a great solution for handling user authentication and access control. With Cognito, you can create different user pools for registered users and guest users. You can assign roles and permissions to these user pools, ensuring that the right users have access to the necessary resources. Cognito also offers unauthenticated (guest) access, making it a suitable choice for this scenario.

#### D. Create a new IAM user for each user and grant read access.

Explanation: Creating a new IAM user for each of the 25,000 users is not a practical approach in terms of scalability and management. It would be challenging to manage access and permissions for a large number of individual IAM users. This approach is not recommended due to its complexity and scalability concerns.

#### E. Use the AWS IAM service and let the application assume the different roles using the AWS Security Token Service (AWS STS) AssumeRole action depending on the type of user and provide read access to Amazon S3 using the assumed role.

Explanation: This is a recommended approach for handling access control based on user types. With this approach, you can create IAM roles for registered users and guest users, and use the AWS STS AssumeRole action in your application to acquire temporary credentials for these roles based on the user type. This ensures that users are granted the appropriate permissions when accessing S3 objects. It is a secure and scalable way to differentiate access based on user types.

In summary, options C and E are the recommended approaches for providing access to both registered and guest users while maintaining security and scalability. Option B is a good practice for fine-grained access control but might require additional complexity for user type differentiation. Options A and D are not recommended due to security and scalability concerns.

---

### Question 147

Let's provide detailed explanations for each of the options in this AWS scenario:

#### A. Use Amazon VPC and keep all resources inside the VPC, and use a VPC link for the S3 bucket with the bucket policy.

Explanation: This approach suggests using a Virtual Private Cloud (VPC) to isolate resources, which is a good practice for enhancing security. However, it doesn't directly address the requirement of providing authorized access to individual employees while ensuring they can access their own application data only. VPCs are more about network isolation and control rather than user-level access control.

#### B. Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy.

Explanation: This is a viable option and provides a comprehensive solution. Amazon Cognito user pools allow you to manage user identities, and by federating with the SAML provider, you can integrate with the company's legacy SAML employee directory. User pool groups can be used to categorize users, and an IAM policy can be associated with each group to control access to AWS resources, including Amazon S3 and Amazon RDS. This approach ensures that each employee can access their own application data based on their group or role within the Cognito user pool.

#### C. Use an Amazon Cognito identity pool, federate with the SAML provider, and use an IAM condition key with a value for the cognitoidentity.amazonaws.com:sub variable to grant access to the employees.

Explanation: This is the correct and recommended approach. Amazon Cognito identity pools provide temporary AWS credentials to users, and by federating with the SAML provider, you can integrate with the company's SAML employee directory. The use of an IAM condition key based on the `cognitoidentity.amazonaws.com:sub` variable allows you to control access at a fine-grained level. This means that each employee can only access their own application data, as the "sub" value represents the unique identifier for the authenticated user. This solution provides secure and granular access control.

#### D. Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only.

Explanation: Creating a unique IAM role for each of the 25,000 employees is neither practical nor scalable. Managing a large number of roles would be extremely complex. Additionally, IAM roles are typically used for more high-level permissions and are not designed for fine-grained data access. This approach would not be suitable for granting access to individual employees' application data.

In summary, option C is the correct choice because it provides fine-grained access control by utilizing Amazon Cognito identity pools and IAM condition keys based on the user's "sub" value. Option B is also a valid approach, as it leverages Amazon Cognito user pools and IAM policies to manage user access. Options A and D are not suitable for achieving the required access control and scalability.

---

### Question 148

Let's provide detailed explanations for each option:

#### A. Compress the application to a .zip file and upload it into AWS Lambda.

Explanation: This option is not the correct step to be completed prior to deploying a serverless application using AWS SAM. While AWS Lambda functions are typically deployed using code packages in the form of .zip files, the AWS SAM CLI abstracts much of this process for you. You don't need to manually compress and upload the code yourself. The correct step would involve using AWS SAM CLI's built-in commands for packaging and deploying the serverless application.

#### B. Test the new AWS Lambda function by first tracing it in AWS X-Ray.

Explanation: While testing your Lambda functions with tracing in AWS X-Ray is a good practice, it's not the specific step that needs to be completed before deploying the application. Tracing and testing should be part of the development and debugging process. You would typically do this before deployment, but it's not a prerequisite for deployment.

#### C. Bundle the serverless application using a SAM package.

Explanation: This is the correct step to complete prior to deploying a serverless application using AWS SAM. The "sam package" command is used to package your application and its dependencies, creating a deployment package that can be deployed to AWS CloudFormation. It takes care of packaging your Lambda function code, uploading it to an S3 bucket, and generating a CloudFormation template. Once this step is done, you can use the "sam deploy" command to deploy your application.

#### D. Create the application environment using the eb create my-env command.

Explanation: The "eb create" command is part of AWS Elastic Beanstalk, which is a Platform-as-a-Service (PaaS) offering for deploying web applications, not a serverless application using AWS SAM. This option is unrelated to deploying a serverless application using AWS Lambda and SAM.

In summary, the correct step to complete before deploying a serverless application using AWS SAM is option C, which involves bundling the serverless application using the "sam package" command. This step prepares your application for deployment using AWS SAM CLI.

---

### Question 149

Let's provide detailed explanations for each of the options in this scenario:

#### A. Use server-side encryption with Amazon S3-managed keys.

Explanation: Server-side encryption with Amazon S3-managed keys is a method where Amazon S3 automatically handles the encryption keys for you. The encryption keys are generated and managed by AWS, not on your on-premises data center. This option is suitable when you want AWS to handle the encryption process and don't need to integrate your on-premises keys. It simplifies the management of encryption but does not involve your on-premises data center in key management.

#### B. Use server-side encryption with AWS KMS-managed keys.

Explanation: Server-side encryption with AWS Key Management Service (KMS)-managed keys allows you to use AWS KMS for managing encryption keys. In this case, the keys are managed by AWS KMS, and not in your on-premises data center. While AWS KMS provides more control and auditing capabilities over key management, it does not directly involve your on-premises data center. This option is suitable when you prefer AWS to manage the keys but with the added control and auditability provided by AWS KMS.

#### C. Use client-side encryption with customer master keys.

Explanation: Client-side encryption involves encrypting data on your own premises using your own encryption keys (customer master keys) before sending it to S3. In this scenario, you would need to manage the encryption process and keys on your on-premises infrastructure. This option provides maximum control over key management but also requires you to handle encryption on your own. It does not leverage S3's server-side encryption.

#### D. Use server-side encryption with customer-provided keys.

Explanation: Server-side encryption with customer-provided keys allows you to provide your own encryption keys for use in Amazon S3. You would manage the encryption keys in your on-premises data center and provide them when uploading data to S3. This option provides control over key management but requires integration with your on-premises key management system. It is suitable when you want to leverage your own keys and maintain control over them.

In summary, option D (server-side encryption with customer-provided keys) provides control and on-premises key management.

---

### Question 150

Let's provide detailed explanations for each of the options in this scenario:

#### A. Develop an AWS Lambda function to check the upload folder in the S3 bucket. If newly uploaded pictures are detected, the Lambda function will scan and parse them.

Explanation: While this option utilizes AWS Lambda to scan and parse pictures, it doesn't make use of S3 event triggers, which can efficiently handle the "smoothing out" of temporary volume spikes. Additionally, the Lambda function would need to continuously poll the S3 bucket, which is not a cost-effective or efficient way to handle large-scale and unpredictable events.

#### B. Once a picture is uploaded to Amazon S3, publish the event to an Amazon SQS queue. Use the queue as an event source to trigger an AWS Lambda function. In the Lambda function, scan and parse the picture.

Explanation: This option is a resilient approach to achieve the goal. When a picture is uploaded to S3, you can configure S3 event notifications to publish events to an Amazon SQS queue. AWS Lambda can then be triggered by SQS messages, which effectively handles temporary volume spikes and scales automatically. This is a decoupled and highly scalable approach for processing events and is a good choice for handling a large number of concurrent picture uploads.

#### C. When the user uploads a picture, invoke an API hosted in Amazon API Gateway. The API will invoke an AWS Lambda function to scan and parse the picture.

Explanation: While this option allows for using API Gateway and AWS Lambda for processing, it doesn't directly address the issue of temporary volume spikes. API Gateway can help you manage and expose APIs, but it doesn't inherently provide mechanisms for handling asynchronous event-driven processing like S3 event triggers combined with SQS.

#### D. Create a state machine in AWS Step Functions to check the upload folder in the S3 bucket. If a new picture is detected, invoke an AWS Lambda function to scan and parse it.

Explanation: AWS Step Functions is a powerful service for orchestrating workflows and is typically used for complex, multi-step processes. However, for the scenario described, it might introduce unnecessary complexity. Using S3 event triggers combined with SQS for AWS Lambda execution is more straightforward and better suited for this task.

In summary, option B (S3 events triggering an SQS queue that invokes AWS Lambda) is the most resilient and efficient approach to handle picture uploads and processing, as it can smoothly handle temporary volume spikes during a large event.

[🔼 Back to top](#question-102)